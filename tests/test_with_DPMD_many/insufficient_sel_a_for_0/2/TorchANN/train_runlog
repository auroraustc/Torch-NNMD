cuDNN version:  7401
Number of GPUs:  1
Using /tmp/torch_extensions as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /tmp/torch_extensions/test_from_cpp/build.ninja...
Building extension module test_from_cpp...
ninja: no work to do.
Loading extension module test_from_cpp...
All parameters:
>>> cutoff_1: 7.700000e+00
>>> cutoff_2: 8.000000e+00
>>> cutoff_3: 0.000000e+00
>>> cutoff_max: 8.000000e+00
>>> N_types_all_frame:  2
>>> type_index_all_frame: [15, 79]
>>> N_Atoms_max: 16
>>> SEL_A_max: 314
>>> Nframes_tot:  5
>>> sym_coord_type:  1
>>> N_sym_coord: 1256
>>> batch_size:  5
>>> stop_epoch:  1
>>> num_filter_layer:  3
>>> filter_neuron: [106, 66, 57]
>>> axis_neuron:  3
>>> num_fitting_layer:  4
>>> fitting_neuron: [230, 220, 117, 91]
>>> start_lr: 5.000000e-04
>>> (Not used)decay_steps: -1
>>> decay_epoch:  1
>>> decay_rate: 9.500000e-01
>>> start_pref_e: 1.000000e+00
>>> limit_pref_e: 1.000000e+00
>>> start_pref_f: 1.000000e+04
>>> limit_pref_f: 1.000000e+00
>>> check_step: 1000
>>> output_epoch: 10
>>> save_epoch: 10

Total number of frames:  5
Number of atoms aligned:  16
COORD_Reshape: shape =  (5, 48)
FORCE_Reshape: shape =  (5, 48)
TYPE_Reshape: shape =  (5, 16)
NEI_IDX_Reshape: shape =  (5, 5024)
NEI_COORD_Reshape: shape =  (5, 15072)
NEI_TYPE_Reshape: shape =  (25120,)
Data pre-processing complete. Building net work.

one_batch_net(
  (batch_norm): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (filter_input): ModuleList(
    (0): ModuleList(
      (0): Linear(in_features=1, out_features=106, bias=True)
      (1): Linear(in_features=1, out_features=106, bias=True)
    )
    (1): ModuleList(
      (0): Linear(in_features=1, out_features=106, bias=True)
      (1): Linear(in_features=1, out_features=106, bias=True)
    )
  )
  (filter_hidden): ModuleList(
    (0): ModuleList(
      (0): ModuleList(
        (0): Linear(in_features=106, out_features=66, bias=True)
        (1): Linear(in_features=66, out_features=57, bias=True)
      )
      (1): ModuleList(
        (0): Linear(in_features=106, out_features=66, bias=True)
        (1): Linear(in_features=66, out_features=57, bias=True)
      )
    )
    (1): ModuleList(
      (0): ModuleList(
        (0): Linear(in_features=106, out_features=66, bias=True)
        (1): Linear(in_features=66, out_features=57, bias=True)
      )
      (1): ModuleList(
        (0): Linear(in_features=106, out_features=66, bias=True)
        (1): Linear(in_features=66, out_features=57, bias=True)
      )
    )
  )
  (fitting_input): ModuleList(
    (0): Linear(in_features=171, out_features=230, bias=True)
    (1): Linear(in_features=171, out_features=230, bias=True)
  )
  (fitting_hidden): ModuleList(
    (0): ModuleList(
      (0): Linear(in_features=230, out_features=220, bias=True)
      (1): Linear(in_features=220, out_features=117, bias=True)
      (2): Linear(in_features=117, out_features=91, bias=True)
    )
    (1): ModuleList(
      (0): Linear(in_features=230, out_features=220, bias=True)
      (1): Linear(in_features=220, out_features=117, bias=True)
      (2): Linear(in_features=117, out_features=91, bias=True)
    )
  )
  (fitting_out): ModuleList(
    (0): Linear(in_features=91, out_features=1, bias=True)
    (1): Linear(in_features=91, out_features=1, bias=True)
  )
)
Number of parameters in the net: 298538
Start training using device:  cuda , count:  1
LR update: lr = 0.000500
Force check:
 tensor([[ 1.4610909963e-02,  4.9060581331e-02,  4.4451764356e-02,
          1.9019224095e-02,  1.1831030018e-02, -4.2408654408e-02,
          5.7676720780e-02, -2.3585612877e-02,  2.0247611283e-02,
         -6.2235577345e-02,  2.3835804310e-02, -9.1217857948e-03,
          4.7746374771e-02,  1.8496466770e-02,  1.7729899985e-02,
         -4.3394835091e-02, -6.2295124155e-02,  1.6913747340e-02,
          4.3913733103e-02,  3.3293520116e-02,  3.7373872537e-02,
         -3.6939200671e-02,  1.0521356029e-02,  7.3307255695e-03,
          3.2658557615e-02, -2.9548882941e-02, -6.3196754418e-04,
          5.1747495067e-02,  1.5230153211e-03, -1.6669692535e-02,
          2.9449110887e-02,  6.2230295332e-03,  3.0753389489e-02,
         -3.9269679139e-02,  8.1771603632e-03, -4.9922986965e-03,
         -4.2779115602e-02,  1.6030687204e-02, -2.2861203420e-02,
          2.2965684776e-03, -1.9751153305e-03, -3.8731661516e-02,
         -3.7224066829e-02, -2.4991397913e-02, -2.7524185143e-02,
         -3.7276220083e-02, -3.6596517779e-02, -1.1859561503e-02],
        [ 1.6306758767e-02,  7.0115148532e-02,  5.2253815309e-02,
          2.0661504946e-02,  6.6027202565e-03, -1.9919952597e-02,
          7.5471710977e-02, -1.1545511500e-02,  2.5998905259e-02,
         -8.9067106352e-02,  5.3916139385e-03,  7.4855468167e-03,
         -1.5498608027e-02,  2.2867874380e-02, -2.7276959109e-02,
         -9.2383185100e-03, -6.2376785059e-02,  7.2337022449e-02,
         -2.9477002593e-02,  1.4096242430e-02,  4.7966782196e-02,
          1.7169449596e-02, -7.1913021567e-03,  6.0004674118e-02,
         -5.4514470874e-02,  4.9162310901e-02, -3.3131432987e-02,
          5.1026715799e-02, -1.3024676002e-02, -2.7482039531e-02,
         -4.0638405416e-02,  3.0732097182e-02, -4.0854394543e-02,
          5.9080112426e-02, -2.8637266905e-02,  9.7299394735e-03,
         -8.8478854012e-04,  1.0922243744e-02, -5.6844555321e-02,
         -5.3430060798e-04, -4.9275302149e-02, -4.2421882439e-02,
         -2.6664523077e-02,  8.0224523535e-03, -2.5110229611e-03,
          2.6801271485e-02, -4.5861859947e-02, -2.5334446131e-02],
        [ 4.8298445557e-02,  1.5531227587e-02,  3.8292816089e-02,
          2.4834744632e-02, -1.9156793603e-03, -4.4075673818e-02,
          1.7266144328e-02, -6.2641525072e-02,  1.7423536183e-02,
         -2.1223727144e-02,  6.3980229135e-02, -1.7015172473e-02,
          5.0460141808e-02, -2.3103390189e-02,  2.3269951183e-02,
         -7.7776369132e-02, -6.0135969319e-03,  2.2819090508e-02,
          5.3301918460e-02, -1.0550688082e-02,  2.8625973739e-02,
         -2.2413868612e-02,  3.3399655488e-02,  1.0467159573e-03,
         -9.6311932822e-03, -4.5462646712e-02,  3.8236636715e-03,
          3.0602917788e-02, -3.5254956646e-02, -1.9888916592e-02,
          2.5859161133e-02, -1.7086016258e-02,  3.0285749921e-02,
         -2.7888086712e-02,  3.4948487073e-02,  7.0448950393e-04,
         -2.1217974466e-02,  4.0717996198e-02, -2.6247984692e-02,
         -1.7776063275e-03,  2.4110207212e-03, -4.0435043021e-02,
         -4.5480240835e-02,  1.1504121575e-02, -1.9067187986e-02,
         -2.3214407194e-02, -4.6423852608e-04,  4.3799182908e-04],
        [ 5.2962146786e-02,  2.5707944723e-02,  3.0792049889e-02,
          4.3980318754e-02, -4.5619828232e-02,  3.7577150480e-02,
         -3.5436555815e-02,  1.4129648269e-02, -5.1577354184e-02,
         -4.0041924803e-02,  3.1254785060e-03,  3.4758656095e-03,
          6.6905375772e-02,  1.6008401258e-02,  5.1101477364e-03,
         -6.3517087947e-02,  2.7218711857e-02, -2.0556451379e-02,
         -6.9191395828e-05,  1.8206984047e-03, -7.8818977216e-02,
          4.3224101378e-02,  1.2118665322e-03, -4.7464356494e-02,
         -2.5715304341e-02,  6.6840436168e-03,  2.8672853302e-02,
          1.0899634603e-02,  3.6226194098e-02,  3.4006899851e-02,
         -3.5730293903e-02,  1.0507006540e-02,  5.3565340508e-02,
         -2.8124751866e-02,  1.3444704669e-02,  7.6769628442e-02,
         -9.4335944156e-03, -3.4346549824e-02, -1.7102696517e-02,
          5.5805974883e-03, -3.9127918767e-02,  1.1067304059e-02,
          6.3561563656e-03, -1.4228786627e-03, -7.4772151441e-02,
          8.1603733370e-03, -3.5567522987e-02,  9.2547473530e-03],
        [-3.1700614347e-03,  4.9412415834e-05, -1.7795164904e-02,
         -1.2741025547e-02, -1.0357129956e-02, -5.3106138515e-02,
          1.1871593921e-03, -1.1386330538e-02, -8.5157383747e-02,
          6.4243050330e-03, -1.4036497592e-02,  9.0074721864e-02,
          1.5265844528e-02,  9.6137808358e-04, -4.8762394126e-02,
          8.9447481135e-03, -3.9346679543e-03, -7.8625454226e-04,
         -5.1621119607e-03,  1.1803042821e-03,  7.5951363765e-02,
          1.7732691775e-02,  1.0208416573e-02, -8.0147763628e-02,
         -1.3511016893e-02, -5.1160328318e-03, -1.2091568643e-02,
         -1.0237445963e-02,  5.0136500789e-02, -6.9875177967e-02,
          7.0937924761e-03, -1.6164497345e-02,  2.2958327127e-02,
          2.6467163258e-02,  4.7025661498e-03,  5.7115658704e-02,
         -9.9218531798e-03,  2.3873222847e-02,  3.1661935660e-02,
         -1.3232364352e-03, -2.5172999509e-02,  4.9654640589e-02,
         -1.4875719945e-02, -3.1891354081e-04,  1.4827901815e-02,
         -1.2173233217e-02, -4.6247318742e-03,  2.5477296549e-02]],
       device='cuda:0')
Additional parameters check:
 std:
 tensor([[[0.0787494605, 0.0487278497, 0.0487278497, 0.0487278497],
         [0.0747274838, 0.0462697329, 0.0462697329, 0.0462697329]]],
       device='cuda:0') 
avg:
 tensor([[[0.0303600904, 0.0000000000, 0.0000000000, 0.0000000000],
         [0.0289620101, 0.0000000000, 0.0000000000, 0.0000000000]]],
       device='cuda:0') 
use_std_avg True
Epoch: 0         , Batch: 0         , lossE:   4.198131 eV/atom, lossF:   0.234188 eV/A, time:      0.050 s
Rank 0: Model saved to ./freeze_model.pytorch
Training complete. Time elapsed:      0.067 s

