2019-07-12 22:31:47.050309: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-12 22:31:47.097756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-12 22:31:47.098001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: GeForce GTX TITAN major: 3 minor: 5 memoryClockRate(GHz): 0.98
pciBusID: 0000:01:00.0
totalMemory: 5.94GiB freeMemory: 5.42GiB
2019-07-12 22:31:47.098029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2019-07-12 22:31:47.278735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-12 22:31:47.278776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2019-07-12 22:31:47.278782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2019-07-12 22:31:47.278863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5216 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:01:00.0, compute capability: 3.5)
# DEEPMD:  _____               _____   __  __  _____           _     _  _   
# DEEPMD: |  __ \             |  __ \ |  \/  ||  __ \         | |   (_)| |  
# DEEPMD: | |  | |  ___   ___ | |__) || \  / || |  | | ______ | | __ _ | |_ 
# DEEPMD: | |  | | / _ \ / _ \|  ___/ | |\/| || |  | ||______|| |/ /| || __|
# DEEPMD: | |__| ||  __/|  __/| |     | |  | || |__| |        |   < | || |_ 
# DEEPMD: |_____/  \___| \___||_|     |_|  |_||_____/         |_|\_\|_| \__|
# DEEPMD: 
# DEEPMD: Please read and cite:
# DEEPMD: Wang, Zhang, Han and E, Comput.Phys.Comm. 228, 178-184 (2018)
# DEEPMD: 
# DEEPMD: ---Summary of the training---------------------------------------
# DEEPMD: installed to:       /home/aurora/Softwares/deepmd
# DEEPMD: source :            v0.12.1
# DEEPMD: source brach:       master
# DEEPMD: source commit:      5caff7d
# DEEPMD: source commit at:   2018-12-06 11:03:13 +0800
# DEEPMD: build float prec:   double
# DEEPMD: build with tf inc:  /home/aurora/Softwares/tensorflow-1.8/include
# DEEPMD: build with tf lib:  /home/aurora/Softwares/tensorflow-1.8/lib/libtensorflow_cc.so
# DEEPMD:                     /home/aurora/Softwares/tensorflow-1.8/lib/libtensorflow_framework.so
# DEEPMD: running on:         aurora-System-Product-Name
# DEEPMD: gpu per node:       None
# DEEPMD: num_inter_threads:  1
# DEEPMD: num_intra_threads:  1
# DEEPMD: -----------------------------------------------------------------
# DEEPMD: 
# DEEPMD: ---Summary of DataSystem-----------------------------------------
# DEEPMD: find 1 system(s):
# DEEPMD:                                     system  natoms  bch_sz  n_bch
# DEEPMD:                                   systems/      31       4      1
# DEEPMD: -----------------------------------------------------------------
# DEEPMD: 
2019-07-12 22:31:47.288702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2019-07-12 22:31:47.288733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-12 22:31:47.288739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2019-07-12 22:31:47.288744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2019-07-12 22:31:47.288795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5216 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:01:00.0, compute capability: 3.5)
mesh: [array([0, 0, 0, 2, 2, 2], dtype=int32)]
descrpt_deriv.shape: (4, 197160)
d_all: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]] (4, 65720)
Start, end 0 0
dd: []
deriv_debug_cur_type: []
ddr: []
dda: []
from inside:sumr, suma, sumn, sumr2, suma2 0.0 0.0 0 0.0 0.0
Start, end 0 65720
dd: [[0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]]
deriv_debug_cur_type: [[0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]]
ddr: [[0.]
 [0.]
 [0.]
 ...
 [0.]
 [0.]
 [0.]]
dda: [[0. 0. 0.]
 [0. 0. 0.]
 [0. 0. 0.]
 ...
 [0. 0. 0.]
 [0. 0. 0.]
 [0. 0. 0.]]
from inside:sumr, suma, sumn, sumr2, suma2 2441.0430625534955 2.0354088784794536e-16 65720 489.85251060247515 163.28417020082506
sumr, suma, sumn, sumr2, suma2 first: [[0.0, 2441.0430625534955]] [[0.0, 2.0354088784794536e-16]] [[0, 65720]] [[0.0, 489.85251060247515]] [[0.0, 163.28417020082506]]
sumr, suma, sumn, sumr2, suma2: [   0.         2441.04306255] [0.00000000e+00 2.03540888e-16] [    0 65720] [  0.        489.8525106] [  0.        163.2841702]
ntypes: 2
dstdunit: [nan, nan, nan, nan]
davgunit: [nan, 0, 0, 0]
ntypes: 2
dstdunit: [0.07793600565178727, 0.049845190986981716, 0.049845190986981716, 0.049845190986981716]
davgunit: [0.03714307764080182, 0, 0, 0]
# DEEPMD: computed coord/descrpt stats
sys_tynatom [[ 0. 31.]]
sys_ener [-98.9280645]
Energy_shift: [ 0.         -3.19122789]
# DEEPMD: computed energy bias
dstd: [[       nan        nan        nan ...        nan        nan        nan]
 [0.07793601 0.04984519 0.04984519 ... 0.04984519 0.04984519 0.04984519]]
# DEEPMD: built lr
DS_LAYER: w:stddev 1.0 outputs_size: 97 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 107 97 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 108 107 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 97 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 107 97 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 108 107 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 540 246 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 246 113 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 113 131 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 131 48 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 48 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 97 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 107 97 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 108 107 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 97 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 107 97 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 108 107 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 540 246 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 246 113 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 113 131 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 131 48 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 48 1 b:stddev 1.0 bavg -3.1912278870967743
bias_energy_e [ 0.         -3.19122789]
DS_LAYER: w:stddev 1.0 outputs_size: 97 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 107 97 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 108 107 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 97 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 107 97 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 108 107 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 540 246 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 246 113 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 113 131 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 131 48 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 48 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 97 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 107 97 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 108 107 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 97 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 107 97 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 108 107 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 540 246 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 246 113 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 113 131 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 131 48 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 48 1 b:stddev 1.0 bavg -3.1912278870967743
bias_energy_e [ 0.         -3.19122789]
DS_LAYER: w:stddev 1.0 outputs_size: 97 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 107 97 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 108 107 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 97 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 107 97 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 108 107 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 540 246 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 246 113 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 113 131 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 131 48 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 48 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 97 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 107 97 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 108 107 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 97 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 107 97 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 108 107 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 540 246 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 246 113 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 113 131 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 131 48 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 48 1 b:stddev 1.0 bavg -3.1912278870967743
bias_energy_e [ 0.         -3.19122789]
dddddddddddddddddddd
0
# DEEPMD: built network
(1, 97)
(1, 97)
(97, 107)
(1, 107)
(107, 108)
(1, 108)
(1, 97)
(1, 97)
(97, 107)
(1, 107)
(107, 108)
(1, 108)
(540, 246)
(246,)
(246, 113)
(113,)
(113, 131)
(131,)
(131, 48)
(48,)
(48, 1)
(1,)
(1, 97)
(1, 97)
(97, 107)
(1, 107)
(107, 108)
(1, 108)
(1, 97)
(1, 97)
(97, 107)
(1, 107)
(107, 108)
(1, 108)
(540, 246)
(246,)
(246, 113)
(113,)
(113, 131)
(131,)
(131, 48)
(48,)
(48, 1)
(1,)
total number of trainable variables:  454008
# DEEPMD: built training
2019-07-12 22:31:49.728988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2019-07-12 22:31:49.729024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-12 22:31:49.729030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2019-07-12 22:31:49.729035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2019-07-12 22:31:49.729081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5216 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:01:00.0, compute capability: 3.5)
# DEEPMD: initialize model from scratch
# DEEPMD: start training at lr 5.00e-04 (== 5.00e-04), final lr will be 5.00e-04
Check from c std:1.000000
Check from c std:1.000000
Check from c std:-nan
force_hat_reshape: [[-0.018879829330405513 -0.018706589506962006 -0.03768490626096329 -0.00043053639462582385 -0.022949629743544007 -0.12234831725582947 0.0245075984214803 0.0037181766728334796 0.054495553708578774 0.039895413703711018 0.0039651915687507477 -0.053005709325110013 -0.02492595430113807 -0.065976705633441171 0.052470545381628549 0.0095253560787608368 0.059464135861522167 -0.06204042821484821 -0.041805345649576338 -0.031828162252750432 0.11859945887360475 0.048499045260102479 0.077235927981337937 0.061677859504653723 -0.024242891241935204 -0.069597185183002591 -0.044129005426750759 0.024123218681149212 0.021994717262189621 0.08599765812725535 0.073916986895786074][-0.01253169896399983 -0.037408394374839414 0.042026699601512556 -0.0017838474790406527 0.078241005448264137 0.00283071209650253 0.017142007349154009 0.075026423559012151 -0.028622888322045147 -0.038146689543433969 0.047742702953420306 -0.012020451109158687 -0.033406995540415936 0.024174485375472093 -0.015880704958596694 0.035249054880817164 0.072783299530942272 0.032671921873598596 0.029141549259237374 -0.050743455248251718 -0.013031415406725963 0.061637720022172875 0.10652608189912159 0.03290018616137929 -0.02918119457775973 -0.021261361182831558 -0.075777283193501882 -0.05729398291170739 0.1107033042282585 -0.021624143247671007 -0.025812835884957395][-0.0921787507556492 -0.018253048315759059 0.0118691416585188 -0.13860165760216803 0.04277867766981993 0.023293022667508784 -0.10000378998755603 -0.020345676020941898 -0.016475455082777746 0.0037952862997583259 -0.025709291176848538 -0.040645707101876968 -0.044826657793493888 0.037676405199351351 0.011640814967001966 -0.10898494586232109 -0.031106039272598859 0.044029491094238704 -0.068412702430187544 0.0421460552993545 0.054074956395414472 0.000397100945767525 0.024564884514471502 0.0039435054841966835 -0.01836092842936058 -0.068394163398291186 0.021187024271574012 0.056495200965555013 -0.037013500117160154 -0.015249757990799046 0.050865043348867774]]
l2_force_loss:[0.058617028072897009]
Check from c std:-nan
force_hat_reshape: [[0.007085451639916425 -0.037713397395220248 -0.0023940745035597268 0.012775626626178347 0.035030376421470853 0.037067009715941734 0.005203051726109323 0.057897496486930627 0.032316232506472156 0.004466145973376444 0.085366510730334952 -0.0019545121544848635 -0.035662805009574278 0.013672996997497652 0.05757258365419185 -0.011747577093735031 0.098493745754561282 -0.073021505553863483 -0.037800105009364877 0.0525639854375571 0.023080970604695314 -0.099180983671186065 -0.0435655545911852 -0.0026151093566507328 0.0075976295421555321 -0.098349976697889252 0.013859850030951467 0.02247169830585441 0.032372151716946039 0.0054614407000765043 -0.024811959731587393 -0.010756183674761523 -0.076850480440212407 -0.0036693247457059959 0.021763451132614922 -0.066043428809278271 0.026771229422114021 0.024178815880457786 0.00064491081114592262 0.012263655582147426 0.048980078202515494 -0.031030346999603664 -0.014698036433120286 -0.045768444614900192 0.0998087542317567 0.010176346472796326 0.037943113348964365 0.031395383303943945 -0.0087417583645798026 -0.051950247502297327 -0.046376250637599807 -0.049428797994061344 0.041940147659155359 -0.033330106505878386 0.038881242671910246 -0.043324428174585448 0.066163667934753215 0.045181117964053746 -0.03648413240357859 0.049970675849610152 0.088857831634456208 -0.037795921476510307 0.054704570941840815 -0.095236354255524874 -0.04724218116978636 0.0092061390666722445 -0.056280256457651522 0.03238424612146288 -0.050260175894221347 -0.012222491059999842 -0.016748046582683605 -0.00191331375461374 -0.021453695254735585 0.02197149889357022 0.048141464967199642 0.10454637260187596 -0.069522200887012053 0.014900454042438673 -0.020302038125958894 0.057223541755787075 -0.16210591880181749 -0.025185383071689844 -0.075731056495657173 -0.052616959981048129 0.02618789105985346 -0.038582019970689892 0.076866807479030047 0.049460573631338468 0.017056248320829328 -0.046269838860824107 0.054495701424339078 -0.025304613223898511 0.025621106412935971 -0.040632881434546984 -0.0072869162692953786 -0.041977885640726159 -0.0074442323327553826 0.0051390971823487641 -0.00052018948609537175 -0.033793036322818465 0.031610146190211609 -0.0033055417153140267 0.0043638055639938214 0.025185982816167071 -0.028982283738047347 0.0065035646416743151 -0.0081424266362118587 -0.053127017971950026 0.0091695792070688262 -0.040284493842258587 0.021453642725245737 -0.046184714272424827 -0.0060827723432118836 0.035983969562264989 0.10069572304413561 -0.035847000702647611 -0.00889735750765346 0.0081364164542136533 -0.0035242979150922971 0.046391334475623811 0.009340135898290837 -0.04398826168550888 0.0056831932705944891 -0.013808791428768327][0.013545381061936557 0.04456988800013803 0.017402495201370546 0.0064935858383334674 -0.0407747230214519 0.0095447999481439883 -0.019766122151926871 0.047317801497726464 -0.10276261049002146 0.042064941192122257 -0.012079380816972666 0.027201954422025429 0.011631902567208745 0.020236526823251392 -0.01551846482034472 -0.00087153882889274138 -0.013481706435430608 0.0094832431694752885 -0.010494094617651707 -0.042876015069209045 0.036046944467471163 -0.012036057972325534 -0.046397050129654435 0.01434882675037924 -0.016663414810802982 0.0265331179853904 -0.021875018104980281 -0.0019561868991915358 -0.027184695938018349 0.01815669246542595 0.0043426644327654888 0.056917825609680804 -0.01069001700228862 -0.013401585948793869 0.010059674217310362 -0.051326301085105115 -0.00087829761997245065 0.029794492444428063 0.013836676512895949 0.035696724782020468 -0.039174474406350822 -0.01517864263093927 -0.010784020734642694 0.015353186436580657 -0.026329337157900413 0.019185660364020696 -0.04709405055492847 0.067017859350938527 -0.0048389233151023116 -0.0090488392358930424 -0.002396756293528039 0.046662414650839756 -0.0035454590091096969 -0.0020774512396220723 0.00064433601841106393 0.023811517549601858 -0.0012396878288387894 -0.021715757296377095 0.026554683687222202 0.040009225347379704 0.016359332493520506 0.0078058163917460992 -0.097391969661101627 0.010307870411737443 0.077093949697910308 -0.018800819931537316 0.0668999595609396 -0.03244798655226188 -0.011472496841922118 0.019429552119385754 -9.12602788685199e-05 0.055191319512615737 0.0028688321098873488 -0.035830759828389076 0.012828218569879386 0.0028224420966571307 -0.023110773925177137 0.084402660571280791 -0.039995221454215787 -0.17477388000393659 0.13509221923497439 0.014617893221606235 -0.13070724513764809 0.03058176346241262 -0.043343708797884781 0.020073908732958459 -0.004057254845380328 -0.036339550511697148 0.047772993529568507 -0.019254395699823693 -0.016550248518048281 0.039394449593558242 0.01198345853902782 0.063012188325967278 -0.056013355353333676 -0.019610740962472943 -0.02600849373520377 0.052949173341876571 -0.0089284026524489354 -0.017165744382213986 0.030182236903761817 0.054951812529637975 -0.12889462518091654 0.097698338317822639 0.073506451715605478 0.021080595920412313 -0.070435662968603333 0.014906891446698671 0.0033028007372190296 -0.02968112469818792 0.010226948251772689 -0.0572708033271292 0.021008617323148019 -0.0064160405553040211 -0.031671367528113371 0.053035296128944594 -0.0967685653005422 -0.017110877339463839 0.15686698532871016 -0.026727321129222579 0.031334993509584505 0.067020520685374285 -0.017492588052635014 0.005995793648042866][-0.012486630685793006 -0.037498604967192956 -0.049254788353919235 0.061692831762561073 0.0014505509369380095 0.032138254305484365 -0.050705225254798168 0.0025055689304155355 -0.029055696253224895 0.0012118121839918001 -0.012079131627862711 0.0859733738948603 -0.024547285593606406 -0.095945101835683846 0.024572518856293833 0.073960789318630427 0.014373005196536063 -0.076999369592113079 0.0070997033206576746 -0.10018463489546515 0.073411706292460618 0.074586554502775362 0.079324936696767323 -0.026535781261701724 -0.1496819130413764 -0.03619477122253624 0.080608660587657574 -0.019409554883609712 0.027497034586568762 0.057818840637649332 -0.071725502466659652 -0.018879829330405513 -0.018706589506962006 -0.03768490626096329 -0.00043053639462582385 -0.022949629743544007 -0.12234831725582947 0.0245075984214803 0.0037181766728334796 0.054495553708578774 0.039895413703711018 0.0039651915687507477 -0.053005709325110013 -0.02492595430113807 -0.065976705633441171 0.052470545381628549 0.0095253560787608368 0.059464135861522167 -0.06204042821484821 -0.041805345649576338 -0.031828162252750432 0.11859945887360475 0.048499045260102479 0.077235927981337937 0.061677859504653723 -0.024242891241935204 -0.069597185183002591 -0.044129005426750759 0.024123218681149212 0.021994717262189621 0.08599765812725535 0.073916986895786074 -0.01253169896399983 -0.037408394374839414 0.042026699601512556 -0.0017838474790406527 0.078241005448264137 0.00283071209650253 0.017142007349154009 0.075026423559012151 -0.028622888322045147 -0.038146689543433969 0.047742702953420306 -0.012020451109158687 -0.033406995540415936 0.024174485375472093 -0.015880704958596694 0.035249054880817164 0.072783299530942272 0.032671921873598596 0.029141549259237374 -0.050743455248251718 -0.013031415406725963 0.061637720022172875 0.10652608189912159 0.03290018616137929 -0.02918119457775973 -0.021261361182831558 -0.075777283193501882 -0.05729398291170739 0.1107033042282585 -0.021624143247671007 -0.025812835884957395 -0.0921787507556492 -0.018253048315759059 0.0118691416585188 -0.13860165760216803 0.04277867766981993 0.023293022667508784 -0.10000378998755603 -0.020345676020941898 -0.016475455082777746 0.0037952862997583259 -0.025709291176848538 -0.040645707101876968 -0.044826657793493888 0.037676405199351351 0.011640814967001966 -0.10898494586232109 -0.031106039272598859 0.044029491094238704 -0.068412702430187544 0.0421460552993545 0.054074956395414472 0.000397100945767525 0.024564884514471502 0.0039435054841966835 -0.01836092842936058 -0.068394163398291186 0.021187024271574012 0.056495200965555013 -0.037013500117160154 -0.015249757990799046 0.050865043348867774]]
l2_force_loss:[0.0572477268067106]
Check l2_l_bch from optimizer[368.00118222703804]
2019-07-12 22:31:51.281413: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcupti.so.9.0 locally
Check from c std:-nan
force_hat_reshape: [[0.007085451639916425 -0.037713397395220248 -0.0023940745035597268 0.012775626626178347 0.035030376421470853 0.037067009715941734 0.005203051726109323 0.057897496486930627 0.032316232506472156 0.004466145973376444 0.085366510730334952 -0.0019545121544848635 -0.035662805009574278 0.013672996997497652 0.05757258365419185 -0.011747577093735031 0.098493745754561282 -0.073021505553863483 -0.037800105009364877 0.0525639854375571 0.023080970604695314 -0.099180983671186065 -0.0435655545911852 -0.0026151093566507328 0.0075976295421555321 -0.098349976697889252 0.013859850030951467 0.02247169830585441 0.032372151716946039 0.0054614407000765043 -0.024811959731587393 -0.010756183674761523 -0.076850480440212407 -0.0036693247457059959 0.021763451132614922 -0.066043428809278271 0.026771229422114021 0.024178815880457786 0.00064491081114592262 0.012263655582147426 0.048980078202515494 -0.031030346999603664 -0.014698036433120286 -0.045768444614900192 0.0998087542317567 0.010176346472796326 0.037943113348964365 0.031395383303943945 -0.0087417583645798026 -0.051950247502297327 -0.046376250637599807 -0.049428797994061344 0.041940147659155359 -0.033330106505878386 0.038881242671910246 -0.043324428174585448 0.066163667934753215 0.045181117964053746 -0.03648413240357859 0.049970675849610152 0.088857831634456208 -0.037795921476510307 0.054704570941840815 -0.095236354255524874 -0.04724218116978636 0.0092061390666722445 -0.056280256457651522 0.03238424612146288 -0.050260175894221347 -0.012222491059999842 -0.016748046582683605 -0.00191331375461374 -0.021453695254735585 0.02197149889357022 0.048141464967199642 0.10454637260187596 -0.069522200887012053 0.014900454042438673 -0.020302038125958894 0.057223541755787075 -0.16210591880181749 -0.025185383071689844 -0.075731056495657173 -0.052616959981048129 0.02618789105985346 -0.038582019970689892 0.076866807479030047 0.049460573631338468 0.017056248320829328 -0.046269838860824107 0.054495701424339078 -0.025304613223898511 0.025621106412935971 -0.040632881434546984 -0.0072869162692953786 -0.041977885640726159 -0.0074442323327553826 0.0051390971823487641 -0.00052018948609537175 -0.033793036322818465 0.031610146190211609 -0.0033055417153140267 0.0043638055639938214 0.025185982816167071 -0.028982283738047347 0.0065035646416743151 -0.0081424266362118587 -0.053127017971950026 0.0091695792070688262 -0.040284493842258587 0.021453642725245737 -0.046184714272424827 -0.0060827723432118836 0.035983969562264989 0.10069572304413561 -0.035847000702647611 -0.00889735750765346 0.0081364164542136533 -0.0035242979150922971 0.046391334475623811 0.009340135898290837 -0.04398826168550888 0.0056831932705944891 -0.013808791428768327][0.013545381061936557 0.04456988800013803 0.017402495201370546 0.0064935858383334674 -0.0407747230214519 0.0095447999481439883 -0.019766122151926871 0.047317801497726464 -0.10276261049002146 0.042064941192122257 -0.012079380816972666 0.027201954422025429 0.011631902567208745 0.020236526823251392 -0.01551846482034472 -0.00087153882889274138 -0.013481706435430608 0.0094832431694752885 -0.010494094617651707 -0.042876015069209045 0.036046944467471163 -0.012036057972325534 -0.046397050129654435 0.01434882675037924 -0.016663414810802982 0.0265331179853904 -0.021875018104980281 -0.0019561868991915358 -0.027184695938018349 0.01815669246542595 0.0043426644327654888 0.056917825609680804 -0.01069001700228862 -0.013401585948793869 0.010059674217310362 -0.051326301085105115 -0.00087829761997245065 0.029794492444428063 0.013836676512895949 0.035696724782020468 -0.039174474406350822 -0.01517864263093927 -0.010784020734642694 0.015353186436580657 -0.026329337157900413 0.019185660364020696 -0.04709405055492847 0.067017859350938527 -0.0048389233151023116 -0.0090488392358930424 -0.002396756293528039 0.046662414650839756 -0.0035454590091096969 -0.0020774512396220723 0.00064433601841106393 0.023811517549601858 -0.0012396878288387894 -0.021715757296377095 0.026554683687222202 0.040009225347379704 0.016359332493520506 0.0078058163917460992 -0.097391969661101627 0.010307870411737443 0.077093949697910308 -0.018800819931537316 0.0668999595609396 -0.03244798655226188 -0.011472496841922118 0.019429552119385754 -9.12602788685199e-05 0.055191319512615737 0.0028688321098873488 -0.035830759828389076 0.012828218569879386 0.0028224420966571307 -0.023110773925177137 0.084402660571280791 -0.039995221454215787 -0.17477388000393659 0.13509221923497439 0.014617893221606235 -0.13070724513764809 0.03058176346241262 -0.043343708797884781 0.020073908732958459 -0.004057254845380328 -0.036339550511697148 0.047772993529568507 -0.019254395699823693 -0.016550248518048281 0.039394449593558242 0.01198345853902782 0.063012188325967278 -0.056013355353333676 -0.019610740962472943 -0.02600849373520377 0.052949173341876571 -0.0089284026524489354 -0.017165744382213986 0.030182236903761817 0.054951812529637975 -0.12889462518091654 0.097698338317822639 0.073506451715605478 0.021080595920412313 -0.070435662968603333 0.014906891446698671 0.0033028007372190296 -0.02968112469818792 0.010226948251772689 -0.0572708033271292 0.021008617323148019 -0.0064160405553040211 -0.031671367528113371 0.053035296128944594 -0.0967685653005422 -0.017110877339463839 0.15686698532871016 -0.026727321129222579 0.031334993509584505 0.067020520685374285 -0.017492588052635014 0.005995793648042866][-0.012486630685793006 -0.037498604967192956 -0.049254788353919235 0.061692831762561073 0.0014505509369380095 0.032138254305484365 -0.050705225254798168 0.0025055689304155355 -0.029055696253224895 0.0012118121839918001 -0.012079131627862711 0.0859733738948603 -0.024547285593606406 -0.095945101835683846 0.024572518856293833 0.073960789318630427 0.014373005196536063 -0.076999369592113079 0.0070997033206576746 -0.10018463489546515 0.073411706292460618 0.074586554502775362 0.079324936696767323 -0.026535781261701724 -0.1496819130413764 -0.03619477122253624 0.080608660587657574 -0.019409554883609712 0.027497034586568762 0.057818840637649332 -0.071725502466659652 -0.018879829330405513 -0.018706589506962006 -0.03768490626096329 -0.00043053639462582385 -0.022949629743544007 -0.12234831725582947 0.0245075984214803 0.0037181766728334796 0.054495553708578774 0.039895413703711018 0.0039651915687507477 -0.053005709325110013 -0.02492595430113807 -0.065976705633441171 0.052470545381628549 0.0095253560787608368 0.059464135861522167 -0.06204042821484821 -0.041805345649576338 -0.031828162252750432 0.11859945887360475 0.048499045260102479 0.077235927981337937 0.061677859504653723 -0.024242891241935204 -0.069597185183002591 -0.044129005426750759 0.024123218681149212 0.021994717262189621 0.08599765812725535 0.073916986895786074 -0.01253169896399983 -0.037408394374839414 0.042026699601512556 -0.0017838474790406527 0.078241005448264137 0.00283071209650253 0.017142007349154009 0.075026423559012151 -0.028622888322045147 -0.038146689543433969 0.047742702953420306 -0.012020451109158687 -0.033406995540415936 0.024174485375472093 -0.015880704958596694 0.035249054880817164 0.072783299530942272 0.032671921873598596 0.029141549259237374 -0.050743455248251718 -0.013031415406725963 0.061637720022172875 0.10652608189912159 0.03290018616137929 -0.02918119457775973 -0.021261361182831558 -0.075777283193501882 -0.05729398291170739 0.1107033042282585 -0.021624143247671007 -0.025812835884957395 -0.0921787507556492 -0.018253048315759059 0.0118691416585188 -0.13860165760216803 0.04277867766981993 0.023293022667508784 -0.10000378998755603 -0.020345676020941898 -0.016475455082777746 0.0037952862997583259 -0.025709291176848538 -0.040645707101876968 -0.044826657793493888 0.037676405199351351 0.011640814967001966 -0.10898494586232109 -0.031106039272598859 0.044029491094238704 -0.068412702430187544 0.0421460552993545 0.054074956395414472 0.000397100945767525 0.024564884514471502 0.0039435054841966835 -0.01836092842936058 -0.068394163398291186 0.021187024271574012 0.056495200965555013 -0.037013500117160154 -0.015249757990799046 0.050865043348867774]]
Check from c std:-nan
force_hat_reshape: [[-0.012012072134914131 -0.014805513284936534 -0.033303187415162874 -0.0023361146248520449 -0.021452224749708662 -0.10275856992137131 0.016944671691397323 0.0016087490258740711 0.048605668196398492 0.03365316493183651 0.0077547486643497563 -0.045497690935944295 -0.021377606557563881 -0.055867078129837572 0.041799457366160739 0.010761909768204049 0.049092166852563873 -0.047523702727765529 -0.035594637803639743 -0.022059690007330046 0.10146950349709888 0.036896670937227628 0.065345944219154517 0.055526081843343594 -0.017907290716113518 -0.055643624592341651 -0.040738656033422374 0.018548305248767546 0.019578153141231593 0.074745027115213261 0.060674332239675034][-0.01172722615226574 -0.026926763987772481 0.034932621047728818 -0.0033243525468540439 0.066237740077672691 -0.0016454216158182041 0.013977879272689736 0.057890046658053876 -0.025018635272874463 -0.032019172687752423 0.04501665188087367 -0.00933678636498583 -0.02665489094914093 0.011824131488979038 -0.01005339684216301 0.03146914947090481 0.060823876155083223 0.028784604046842483 0.023654918344251291 -0.044724811550746944 -0.012043170211679886 0.051594871601616035 0.093463310412784009 0.030628555537043908 -0.027378877937123149 -0.016135825897116998 -0.061829348012696288 -0.0470583287201109 0.088864277192083785 -0.018462393633075693 -0.01590978545527489][-0.076759199889393281 -0.016420487848065685 0.012667348181789412 -0.11735991434706308 0.037366782667722628 0.016429032115839626 -0.08407192821275275 -0.017858560736734121 -0.017463824174139091 -0.0029321049065080358 -0.019735262881796327 -0.031280422877037757 -0.040428838500568595 0.030490437973501976 0.0073489243605854553 -0.091520900053891144 -0.023441071365041588 0.032855856449648772 -0.057946828186148845 0.033085186290603377 0.044291619998533212 0.0054500750013191145 0.018741727312125724 -0.00071583903590893394 -0.015142146000558113 -0.059736148201793279 0.018572234211461376 0.048627133336129176 -0.026700564868869479 -0.01288074461073101 0.04342808834499292]]
l2_force_loss:[0.058358860526648924]
Check from c std:-nan
force_hat_reshape: [[0.0061966451306021044 -0.034109440948077427 -0.0039867678795718429 0.012937046282731585 0.027685154552103325 0.030682839425691222 0.0029480725264108979 0.048847122669714076 0.030009101745434457 0.0012471142845163507 0.070833200803281371 -0.00562114545068239 -0.029924895667262995 0.013001321097638614 0.044712699290449749 -0.0098635234864394555 0.083230958985253761 -0.064802237213261157 -0.035103607910583444 0.045323804284645587 0.018118718437314749 -0.084298608312778744 -0.034822251582443869 0.0026505257760336513 0.0034919074590871574 -0.085644297525089647 0.015285579101972705 0.022803313592576816 0.024750715561160121 0.0037659941323044538 -0.018693371435108047 -0.012124431396953815 -0.06762125035047159 -0.0035723553642240213 0.016870458699714146 -0.054532933121775144 0.025443976094079956 0.021079442723006731 0.0058553049673250208 0.0080390046888865717 0.045012744614646112 -0.026067011341861974 -0.011275450775084534 -0.037604886214846207 0.085734084702487626 0.011391638480658512 0.03591030257000416 0.025353031876967514 -0.011874938588108312 -0.04080982671022778 -0.040990966348490157 -0.042413744513356444 0.035642525931240218 -0.025505907880355453 0.031034370666434653 -0.034082078872235251 0.056201813995986519 0.0383945310482921 -0.028386300269221475 0.042043221971994109 0.0759299402597408 -0.03304486494754337 0.045764967087861846 -0.079235954735951561 -0.041011353636352346 0.0036897600209575389 -0.047386665048052688 0.027598541284888804 -0.043825060527943972 -0.0096954491082339913 -0.013991065858128655 -0.0025919370403621033 -0.016404795359277427 0.014382227326542332 0.041271770285860251 0.0895197635061772 -0.058991257112144195 0.015172354955085874 -0.018519081041666618 0.043266846294656 -0.13227801921743046 -0.023462366687702945 -0.0648525382413873 -0.043278555444127023 0.024290854730409359 -0.032520634047138612 0.061149636762385252 0.038690411338161861 0.018532711389033278 -0.039920699537980241 0.049366217945065162 -0.019972851425738458 0.023561086818200773 -0.035331681440214223 -0.0038230546837288485 -0.035312935590819661 -0.0078915728133669982 0.0058736760507646316 -0.00029478878649485805 -0.025568993750259988 0.026118383363496603 -0.0051491976199009885 0.0024226937846402803 0.021343542533398954 -0.023576143904215856 0.0056494033813950309 -0.007922166749851656 -0.043233730709117214 0.0063243042202455321 -0.032143057315069994 0.019000683339969807 -0.037950017047517083 -0.0047819495666632638 0.029367096872055486 0.0823419505763452 -0.027786207392563055 -0.0065160601038615632 0.0057372037308986617 -0.0019082468236169884 0.038247282535601022 0.0045214873755018191 -0.034915793856726135 0.0069708341664447265 -0.0093040248431563461][0.009579588714687998 0.035596796970385362 0.013329116986822613 0.0052406379044046712 -0.033337612654291714 0.0090906509476431276 -0.016736889073262071 0.038497681687453822 -0.084358560329618984 0.032686949767577744 -0.010310914305837788 0.024073232506762313 0.0072541813965918776 0.017620146834780602 -0.011980318398988032 -0.0002624791932627929 -0.011079729966687197 0.0056636792925123065 -0.0079671107179999246 -0.033996525722769309 0.027891869008069791 -0.0092180389707928814 -0.03633943953588957 0.01186424679168281 -0.014399989975632188 0.022563576583839336 -0.018382938167127817 -0.0032593391918165618 -0.021723319587667229 0.017666986744254071 0.0028672103208691219 0.045321191573008415 -0.0086855848717894719 -0.010944438898255769 0.008173551900500042 -0.044453354572657977 0.0008658164687745212 0.024639503466856665 0.012231640997181794 0.027245382898807777 -0.032575399880975471 -0.010813980659095524 -0.0094613061867868022 0.011997441203943644 -0.022231093445864308 0.015656954384167057 -0.039048592042962904 0.057422301680141265 -0.0055491708079773447 -0.0077544304443921876 0.0011872168683779072 0.03755239508129652 -0.0054250833015944706 -0.0016966266686425735 0.00051523401272204829 0.019277928117964253 -0.0017823257757268126 -0.015440058802574413 0.022084917562329529 0.0330130878915516 0.013719345309021179 0.0063152713423453221 -0.081047908551638348 0.011230800103130876 0.069507641651381846 -0.014245323485219213 0.053031658969861319 -0.027013581636887635 -0.010881493797673796 0.017764988647248126 0.00081259602371873028 0.045459759026957113 0.00335337149508837 -0.031953796682770805 0.01073251295458074 0.0015163657252468975 -0.0189468407590879 0.070303680863476881 -0.033330747859724529 -0.14681297089369752 0.10890785367288495 0.0096110032384472289 -0.11296349225505917 0.019623694455538274 -0.033709332213601145 0.015943465170136336 -0.0019246136484157997 -0.029416064025428172 0.041684360836226647 -0.013931580380349929 -0.014903106554037241 0.036134632938161812 0.0097512343659925619 0.048174931783305307 -0.044652023343059309 -0.015816194886232326 -0.019992182659083677 0.042646250636583644 -0.0067181725251396008 -0.013910957820743102 0.027709186710278829 0.046838048035982544 -0.10249901319125586 0.082441472866852189 0.061583175617388485 0.013334836611731521 -0.057475842953715384 0.010613719215459722 0.003705800065648576 -0.027847129860663927 0.0089227184799573669 -0.046237209043486376 0.017244183918893922 -0.0052969976703452007 -0.024104891089106906 0.04574239738952264 -0.079373237535001417 -0.010801211289653697 0.13436080237813811 -0.020938818651200976 0.023629078960575835 0.053924286060735654 -0.01216061611739201 0.0028599006202063926][-0.011309727417543656 -0.029591421232093452 -0.038368389086974611 0.048056988495432711 -9.0223814184886726e-05 0.026776422808959321 -0.045070396462274551 0.0024375926500145883 -0.020633283966564806 0.0016712735199686652 -0.011128502321824416 0.069261068743174517 -0.025258459634211035 -0.079384046312193257 0.019484921439844396 0.065468063652181135 0.011599179785726979 -0.063399380609435244 0.0032028266068251984 -0.08300367385241629 0.059941224046216164 0.062871498491215469 0.0667890828045975 -0.022996678546748858 -0.12833046521537489 -0.029187796780495077 0.065476935226606248 -0.014928457664515171 0.02115836963325848 0.045149139470553039 -0.056858742567392311 -0.012012072134914131 -0.014805513284936534 -0.033303187415162874 -0.0023361146248520449 -0.021452224749708662 -0.10275856992137131 0.016944671691397323 0.0016087490258740711 0.048605668196398492 0.03365316493183651 0.0077547486643497563 -0.045497690935944295 -0.021377606557563881 -0.055867078129837572 0.041799457366160739 0.010761909768204049 0.049092166852563873 -0.047523702727765529 -0.035594637803639743 -0.022059690007330046 0.10146950349709888 0.036896670937227628 0.065345944219154517 0.055526081843343594 -0.017907290716113518 -0.055643624592341651 -0.040738656033422374 0.018548305248767546 0.019578153141231593 0.074745027115213261 0.060674332239675034 -0.01172722615226574 -0.026926763987772481 0.034932621047728818 -0.0033243525468540439 0.066237740077672691 -0.0016454216158182041 0.013977879272689736 0.057890046658053876 -0.025018635272874463 -0.032019172687752423 0.04501665188087367 -0.00933678636498583 -0.02665489094914093 0.011824131488979038 -0.01005339684216301 0.03146914947090481 0.060823876155083223 0.028784604046842483 0.023654918344251291 -0.044724811550746944 -0.012043170211679886 0.051594871601616035 0.093463310412784009 0.030628555537043908 -0.027378877937123149 -0.016135825897116998 -0.061829348012696288 -0.0470583287201109 0.088864277192083785 -0.018462393633075693 -0.01590978545527489 -0.076759199889393281 -0.016420487848065685 0.012667348181789412 -0.11735991434706308 0.037366782667722628 0.016429032115839626 -0.08407192821275275 -0.017858560736734121 -0.017463824174139091 -0.0029321049065080358 -0.019735262881796327 -0.031280422877037757 -0.040428838500568595 0.030490437973501976 0.0073489243605854553 -0.091520900053891144 -0.023441071365041588 0.032855856449648772 -0.057946828186148845 0.033085186290603377 0.044291619998533212 0.0054500750013191145 0.018741727312125724 -0.00071583903590893394 -0.015142146000558113 -0.059736148201793279 0.018572234211461376 0.048627133336129176 -0.026700564868869479 -0.01288074461073101 0.04342808834499292]]
l2_force_loss:[0.057652065250376062]
Check l2_l_bch from optimizer[350.26422773833576]
# DEEPMD: batch       1 training time 0.82 s, testing time 0.34 s
# DEEPMD: saved checkpoint model.ckpt
Start L-BFGS
0
# DEEPMD: finished training
# DEEPMD: wall time: 2.505 s
/home/aurora/PycharmProjects/deepmd_analysis/deepmd_test/bin/../lib/deepmd/Model.py:888: RuntimeWarning: invalid value encountered in double_scalars
  davgunit = [sumr[type_i]/sumn[type_i], 0, 0, 0]
/home/aurora/PycharmProjects/deepmd_analysis/deepmd_test/bin/../lib/deepmd/Model.py:848: RuntimeWarning: invalid value encountered in double_scalars
  return np.sqrt(sumv2/sumn - np.multiply(sumv/sumn, sumv/sumn))
