2019-07-12 22:38:38.887055: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-12 22:38:38.930373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-12 22:38:38.930624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: GeForce GTX TITAN major: 3 minor: 5 memoryClockRate(GHz): 0.98
pciBusID: 0000:01:00.0
totalMemory: 5.94GiB freeMemory: 5.43GiB
2019-07-12 22:38:38.930638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2019-07-12 22:38:39.122586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-12 22:38:39.122621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2019-07-12 22:38:39.122627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2019-07-12 22:38:39.122705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5219 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:01:00.0, compute capability: 3.5)
# DEEPMD:  _____               _____   __  __  _____           _     _  _   
# DEEPMD: |  __ \             |  __ \ |  \/  ||  __ \         | |   (_)| |  
# DEEPMD: | |  | |  ___   ___ | |__) || \  / || |  | | ______ | | __ _ | |_ 
# DEEPMD: | |  | | / _ \ / _ \|  ___/ | |\/| || |  | ||______|| |/ /| || __|
# DEEPMD: | |__| ||  __/|  __/| |     | |  | || |__| |        |   < | || |_ 
# DEEPMD: |_____/  \___| \___||_|     |_|  |_||_____/         |_|\_\|_| \__|
# DEEPMD: 
# DEEPMD: Please read and cite:
# DEEPMD: Wang, Zhang, Han and E, Comput.Phys.Comm. 228, 178-184 (2018)
# DEEPMD: 
# DEEPMD: ---Summary of the training---------------------------------------
# DEEPMD: installed to:       /home/aurora/Softwares/deepmd
# DEEPMD: source :            v0.12.1
# DEEPMD: source brach:       master
# DEEPMD: source commit:      5caff7d
# DEEPMD: source commit at:   2018-12-06 11:03:13 +0800
# DEEPMD: build float prec:   double
# DEEPMD: build with tf inc:  /home/aurora/Softwares/tensorflow-1.8/include
# DEEPMD: build with tf lib:  /home/aurora/Softwares/tensorflow-1.8/lib/libtensorflow_cc.so
# DEEPMD:                     /home/aurora/Softwares/tensorflow-1.8/lib/libtensorflow_framework.so
# DEEPMD: running on:         aurora-System-Product-Name
# DEEPMD: gpu per node:       None
# DEEPMD: num_inter_threads:  1
# DEEPMD: num_intra_threads:  1
# DEEPMD: -----------------------------------------------------------------
# DEEPMD: 
# DEEPMD: ---Summary of DataSystem-----------------------------------------
# DEEPMD: find 1 system(s):
# DEEPMD:                                     system  natoms  bch_sz  n_bch
# DEEPMD:                                   systems/       8       5      1
# DEEPMD: -----------------------------------------------------------------
# DEEPMD: 
2019-07-12 22:38:39.132375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2019-07-12 22:38:39.132416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-12 22:38:39.132422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2019-07-12 22:38:39.132427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2019-07-12 22:38:39.132472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5219 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:01:00.0, compute capability: 3.5)
mesh: [array([0, 0, 0, 2, 2, 3], dtype=int32)]
descrpt_deriv.shape: (5, 53280)
d_all: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]] (5, 17760)
Start, end 0 0
dd: []
deriv_debug_cur_type: []
ddr: []
dda: []
from inside:sumr, suma, sumn, sumr2, suma2 0.0 0.0 0 0.0 0.0
Start, end 0 17760
dd: [[0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]]
deriv_debug_cur_type: [[0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]]
ddr: [[0.]
 [0.]
 [0.]
 ...
 [0.]
 [0.]
 [0.]]
dda: [[0. 0. 0.]
 [0. 0. 0.]
 [0. 0. 0.]
 ...
 [0. 0. 0.]
 [0. 0. 0.]
 [0. 0. 0.]]
from inside:sumr, suma, sumn, sumr2, suma2 294.94025889827924 8.696747026230393e-16 22200 67.3825035683183 22.46083452277276
sumr, suma, sumn, sumr2, suma2 first: [[0.0, 294.94025889827924]] [[0.0, 8.696747026230393e-16]] [[0, 22200]] [[0.0, 67.3825035683183]] [[0.0, 22.46083452277276]]
sumr, suma, sumn, sumr2, suma2: [  0.        294.9402589] [0.00000000e+00 8.69674703e-16] [    0 22200] [ 0.         67.38250357] [ 0.         22.46083452]
ntypes: 2
dstdunit: [nan, nan, nan, nan]
davgunit: [nan, 0, 0, 0]
ntypes: 2
dstdunit: [0.05346719381317392, 0.03180800689806895, 0.03180800689806895, 0.03180800689806895]
davgunit: [0.013285597247670236, 0, 0, 0]
# DEEPMD: computed coord/descrpt stats
sys_tynatom [[0. 8.]]
sys_ener [-20.994665]
Energy_shift: [ 0.         -2.62433312]
# DEEPMD: computed energy bias
dstd: [[       nan        nan        nan ...        nan        nan        nan]
 [0.05346719 0.03180801 0.03180801 ... 0.03180801 0.03180801 0.03180801]]
# DEEPMD: built lr
DS_LAYER: w:stddev 1.0 outputs_size: 130 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 92 130 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 28 92 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 130 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 92 130 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 28 92 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 252 254 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 254 170 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 170 54 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 54 47 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 47 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 130 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 92 130 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 28 92 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 130 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 92 130 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 28 92 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 252 254 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 254 170 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 170 54 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 54 47 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 47 1 b:stddev 1.0 bavg -2.6243331249999997
bias_energy_e [ 0.         -2.62433312]
DS_LAYER: w:stddev 1.0 outputs_size: 130 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 92 130 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 28 92 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 130 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 92 130 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 28 92 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 252 254 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 254 170 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 170 54 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 54 47 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 47 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 130 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 92 130 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 28 92 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 130 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 92 130 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 28 92 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 252 254 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 254 170 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 170 54 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 54 47 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 47 1 b:stddev 1.0 bavg -2.6243331249999997
bias_energy_e [ 0.         -2.62433312]
DS_LAYER: w:stddev 1.0 outputs_size: 130 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 92 130 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 28 92 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 130 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 92 130 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 28 92 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 252 254 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 254 170 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 170 54 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 54 47 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 47 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 130 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 92 130 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 28 92 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 130 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 92 130 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 28 92 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 252 254 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 254 170 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 170 54 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 54 47 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 47 1 b:stddev 1.0 bavg -2.6243331249999997
bias_energy_e [ 0.         -2.62433312]
dddddddddddddddddddd
0
# DEEPMD: built network
(1, 130)
(1, 130)
(130, 92)
(1, 92)
(92, 28)
(1, 28)
(1, 130)
(1, 130)
(130, 92)
(1, 92)
(92, 28)
(1, 28)
(252, 254)
(254,)
(254, 170)
(170,)
(170, 54)
(54,)
(54, 47)
(47,)
(47, 1)
(1,)
(1, 130)
(1, 130)
(130, 92)
(1, 92)
(92, 28)
(1, 28)
(1, 130)
(1, 130)
(130, 92)
(1, 92)
(92, 28)
(1, 28)
(252, 254)
(254,)
(254, 170)
(170,)
(170, 54)
(54,)
(54, 47)
(47,)
(47, 1)
(1,)
total number of trainable variables:  298622
# DEEPMD: built training
2019-07-12 22:38:41.496001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2019-07-12 22:38:41.496035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-12 22:38:41.496040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2019-07-12 22:38:41.496045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2019-07-12 22:38:41.496090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5219 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:01:00.0, compute capability: 3.5)
# DEEPMD: initialize model from scratch
# DEEPMD: start training at lr 5.00e-04 (== 5.00e-04), final lr will be 5.00e-04
Check from c std:1.000000
Check from c std:1.000000
Check from c std:-nan
force_hat_reshape: [[0.0050779443412103144 0.016054451396902366 0.10477017515520265 0.01269860946894279 0.019006995312532287 -0.027571693149380926 -0.00238763168003233 -0.0020408769612134166][0.028790332592076921 -0.012010163493148669 -0.0010404583166427385 -0.079059451351468552 0.030643997589674263 -0.018521914061192628 -0.057492562700732552 -0.00028134518551591595][-0.015660268299643131 0.072497444346005122 -0.037042988274450545 -0.00459095153239036 -0.095223527103699648 0.0033015772333201045 0.006793022461647617 0.053289282211996816]]
l2_force_loss:[0.15845898241943834]
Check from c std:-nan
force_hat_reshape: [[0.0082098947231449022 0.019993072877098627 0.056342245918315707 0.00739803494831086 0.0090282624794969773 -0.099563994600725389 -0.0031596614975756402 -0.0070985744237003481 -0.0687998741802952 0.024043207201228878 -0.006297443383358063 0.014126958333771294 -0.028886990563507541 0.0043832281618877431 -0.0098011050946231783 -0.007170486650462507 0.014853493389080088 0.068977768649345114 0.010774595730504852 -0.024016776465498057 -0.062146088884437907 -0.011208593891643802 -0.010845262635006989 0.1008640898586496 0.001770716839799068 -0.027438932606868718 0.058256355859932205 -0.014746728051129233 0.04079788032222989 0.091881788104123271 0.016314703753761123 -0.046958092071234539 -0.087852692478321115 0.0067047928957723532 0.042591412688314026 0.084242246937494464 0.005628127109050229 -0.021394233905418482 -0.0014568251181243837 0.0020384470857197435][0.024390071018994236 -0.0527845587832603 -0.018805852307208357 -0.033018998886729579 -0.092687080293089766 0.001095792674235072 0.021030893440713159 0.00040076577124569093 0.00856678998135437 -0.014595153999724329 0.046878589734675596 0.0086855393564424128 0.03032708978508333 -0.049471694991433342 0.000807510545777974 0.0074256389777366972 0.061269205739459147 -0.0066896309943754633 0.010067553687297117 -0.098992286805673832 -0.016078337036674949 -0.0077216787569612793 -0.080958257350563112 0.0025311382525779937 -0.014553383818896358 0.087219638446870779 -0.0059456323817856441 -0.01024114303562409 0.050398515959572329 0.00812262227668331 -0.00070892283891107392 -0.016343710732907541 0.0050779443412103144 0.016054451396902366 0.10477017515520265 0.01269860946894279 0.019006995312532287 -0.027571693149380926 -0.00238763168003233 -0.0020408769612134166][0.028790332592076921 -0.012010163493148669 -0.0010404583166427385 -0.079059451351468552 0.030643997589674263 -0.018521914061192628 -0.057492562700732552 -0.00028134518551591595 -0.015660268299643131 0.072497444346005122 -0.037042988274450545 -0.00459095153239036 -0.095223527103699648 0.0033015772333201045 0.006793022461647617 0.053289282211996816 0.038306251329102078 0.011779567580873847 0.014252977630026779 -0.011250604457613635 -0.00099289401164778216 0.085035532780610254 -0.0015989534915559628 -0.0043602952504537915 -0.090073316899874009 -0.0039926654967299994 -0.024550382018276036 0.0511749567278644 -0.014418919442535039 0.032160251577972442 -0.062873828137521126 0.0084307350306779463 0.01228557956474048 0.019963565948316091 0.021742822614157154 -0.024412617059883641 -0.047743547670311325 -0.037218666085502547 -0.0019092103833254345 0.030263659620888991]]
l2_force_loss:[0.083259606299386307]
Check l2_l_bch from optimizer[134.51597407719672]
2019-07-12 22:38:43.020063: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcupti.so.9.0 locally
Check from c std:-nan
force_hat_reshape: [[0.0082098947231449022 0.019993072877098627 0.056342245918315707 0.00739803494831086 0.0090282624794969773 -0.099563994600725389 -0.0031596614975756402 -0.0070985744237003481 -0.0687998741802952 0.024043207201228878 -0.006297443383358063 0.014126958333771294 -0.028886990563507541 0.0043832281618877431 -0.0098011050946231783 -0.007170486650462507 0.014853493389080088 0.068977768649345114 0.010774595730504852 -0.024016776465498057 -0.062146088884437907 -0.011208593891643802 -0.010845262635006989 0.1008640898586496 0.001770716839799068 -0.027438932606868718 0.058256355859932205 -0.014746728051129233 0.04079788032222989 0.091881788104123271 0.016314703753761123 -0.046958092071234539 -0.087852692478321115 0.0067047928957723532 0.042591412688314026 0.084242246937494464 0.005628127109050229 -0.021394233905418482 -0.0014568251181243837 0.0020384470857197435][0.024390071018994236 -0.0527845587832603 -0.018805852307208357 -0.033018998886729579 -0.092687080293089766 0.001095792674235072 0.021030893440713159 0.00040076577124569093 0.00856678998135437 -0.014595153999724329 0.046878589734675596 0.0086855393564424128 0.03032708978508333 -0.049471694991433342 0.000807510545777974 0.0074256389777366972 0.061269205739459147 -0.0066896309943754633 0.010067553687297117 -0.098992286805673832 -0.016078337036674949 -0.0077216787569612793 -0.080958257350563112 0.0025311382525779937 -0.014553383818896358 0.087219638446870779 -0.0059456323817856441 -0.01024114303562409 0.050398515959572329 0.00812262227668331 -0.00070892283891107392 -0.016343710732907541 0.0050779443412103144 0.016054451396902366 0.10477017515520265 0.01269860946894279 0.019006995312532287 -0.027571693149380926 -0.00238763168003233 -0.0020408769612134166][0.028790332592076921 -0.012010163493148669 -0.0010404583166427385 -0.079059451351468552 0.030643997589674263 -0.018521914061192628 -0.057492562700732552 -0.00028134518551591595 -0.015660268299643131 0.072497444346005122 -0.037042988274450545 -0.00459095153239036 -0.095223527103699648 0.0033015772333201045 0.006793022461647617 0.053289282211996816 0.038306251329102078 0.011779567580873847 0.014252977630026779 -0.011250604457613635 -0.00099289401164778216 0.085035532780610254 -0.0015989534915559628 -0.0043602952504537915 -0.090073316899874009 -0.0039926654967299994 -0.024550382018276036 0.0511749567278644 -0.014418919442535039 0.032160251577972442 -0.062873828137521126 0.0084307350306779463 0.01228557956474048 0.019963565948316091 0.021742822614157154 -0.024412617059883641 -0.047743547670311325 -0.037218666085502547 -0.0019092103833254345 0.030263659620888991]]
Check from c std:-nan
force_hat_reshape: [[0.0042095723545573773 0.01388814742293573 0.086489546263393266 0.011948905902559646 0.01621519398669358 -0.02259004772498904 -0.0024101844959910587 -0.0022844424595681824][0.023897867567040879 -0.010149273941025116 -0.00061863484569072975 -0.06531443987759232 0.025953905700740122 -0.015734575379007097 -0.047775020815086208 -0.0026111335854397811][-0.013774398161754822 0.059536702915673825 -0.029947058755832 -0.0033844367185067335 -0.078047956912777952 0.0030052668204308295 0.0056931461548982544 0.043803348584337612]]
l2_force_loss:[0.16003934633158909]
Check from c std:-nan
force_hat_reshape: [[0.0069029452327976507 0.017283382293539106 0.046306057199980655 0.0051082741247051172 0.0069812960735908805 -0.080542290683304679 -0.0029763645891898176 -0.0065520451168957546 -0.0566083614040978 0.018630303153728343 -0.0055715594659650029 0.011737139848237091 -0.02278190192810562 0.0033478393218979345 -0.0085004279130434981 -0.0060445662852745795 0.011489189949079078 0.05575472807710885 0.0092377149622855318 -0.018408727402348321 -0.050873814333690981 -0.0080764046709466478 -0.00856937565289792 0.0827269692088104 4.2117059659906352e-05 -0.021099721902265316 0.047575053424495341 -0.012458342629104564 0.029441482239069117 0.074882985218533843 0.015093652800786056 -0.039289730383116971 -0.072114497708072489 0.0064322445401445768 0.035188988036782075 0.069296649968650453 0.0047733261478276811 -0.015356519488907958 -0.0010414424461434387 0.00082544983059254452][0.019356668746784413 -0.04291615435686489 -0.015499928687871327 -0.023329541771203083 -0.07584806840066792 0.00079148093796511129 0.015088374522857785 0.00016547430006927396 0.0057432827233398695 -0.011780655589832654 0.039006479388703837 0.0069728784866269983 0.024627730980042097 -0.041121228745351186 0.0013051037469422229 0.0061028108069435486 0.050657457010035795 -0.0061582817851499405 0.0072616956163559614 -0.080567965006057285 -0.012813845120341057 -0.00763417817732996 -0.0670102002833334 0.0036100625455567203 -0.010994151127094502 0.071598344094530031 -0.004759893430285449 -0.0070334442843132576 0.041302131200088139 0.0061006928333106371 -0.00054980822477120072 -0.013865017658615876 0.0042095723545573773 0.01388814742293573 0.086489546263393266 0.011948905902559646 0.01621519398669358 -0.02259004772498904 -0.0024101844959910587 -0.0022844424595681824][0.023897867567040879 -0.010149273941025116 -0.00061863484569072975 -0.06531443987759232 0.025953905700740122 -0.015734575379007097 -0.047775020815086208 -0.0026111335854397811 -0.013774398161754822 0.059536702915673825 -0.029947058755832 -0.0033844367185067335 -0.078047956912777952 0.0030052668204308295 0.0056931461548982544 0.043803348584337612 0.03196707062542551 0.0090894310198501253 0.01140377127848053 -0.0094194944803165752 -0.00092540746459597709 0.070380039584315851 -0.0013466996838630309 -0.0036281927320069647 -0.07394176420142072 -0.0045846517171683285 -0.02028474461698616 0.041976138041015548 -0.01209210397221056 0.027009789336459186 -0.051440494409913183 0.0062748804549343567 0.011594814276051907 0.016305085610504529 0.017508197966117498 -0.021008735862567875 -0.039216243065733077 -0.028307199192918894 -0.0018469539562042455 0.024533467162750539]]
l2_force_loss:[0.083032165521859144]
Check l2_l_bch from optimizer[132.58353286913123]
# DEEPMD: batch       1 training time 0.78 s, testing time 0.27 s
# DEEPMD: saved checkpoint model.ckpt
Start L-BFGS
0
# DEEPMD: finished training
# DEEPMD: wall time: 2.350 s
/home/aurora/PycharmProjects/deepmd_analysis/deepmd_test/bin/../lib/deepmd/Model.py:888: RuntimeWarning: invalid value encountered in double_scalars
  davgunit = [sumr[type_i]/sumn[type_i], 0, 0, 0]
/home/aurora/PycharmProjects/deepmd_analysis/deepmd_test/bin/../lib/deepmd/Model.py:848: RuntimeWarning: invalid value encountered in double_scalars
  return np.sqrt(sumv2/sumn - np.multiply(sumv/sumn, sumv/sumn))
