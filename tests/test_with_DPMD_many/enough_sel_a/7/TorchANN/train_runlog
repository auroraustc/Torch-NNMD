cuDNN version:  7401
Number of GPUs:  1
Using /tmp/torch_extensions as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /tmp/torch_extensions/test_from_cpp/build.ninja...
Building extension module test_from_cpp...
ninja: no work to do.
Loading extension module test_from_cpp...
All parameters:
>>> cutoff_1: 7.700000e+00
>>> cutoff_2: 8.000000e+00
>>> cutoff_3: 0.000000e+00
>>> cutoff_max: 8.000000e+00
>>> N_types_all_frame:  2
>>> type_index_all_frame: [15, 79]
>>> N_Atoms_max: 47
>>> SEL_A_max: 575
>>> Nframes_tot:  4
>>> sym_coord_type:  1
>>> N_sym_coord: 2300
>>> batch_size:  4
>>> stop_epoch:  1
>>> num_filter_layer:  3
>>> filter_neuron: [64, 66, 32]
>>> axis_neuron:  5
>>> num_fitting_layer:  4
>>> fitting_neuron: [297, 238, 125, 40]
>>> start_lr: 5.000000e-04
>>> (Not used)decay_steps: -1
>>> decay_epoch:  1
>>> decay_rate: 9.500000e-01
>>> start_pref_e: 1.000000e+00
>>> limit_pref_e: 1.000000e+00
>>> start_pref_f: 1.000000e+04
>>> limit_pref_f: 1.000000e+00
>>> check_step: 1000
>>> output_epoch: 10
>>> save_epoch: 10

Total number of frames:  4
Number of atoms aligned:  47
COORD_Reshape: shape =  (4, 141)
FORCE_Reshape: shape =  (4, 141)
TYPE_Reshape: shape =  (4, 47)
NEI_IDX_Reshape: shape =  (4, 27025)
NEI_COORD_Reshape: shape =  (4, 81075)
NEI_TYPE_Reshape: shape =  (108100,)
Data pre-processing complete. Building net work.

one_batch_net(
  (batch_norm): BatchNorm1d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (filter_input): ModuleList(
    (0): ModuleList(
      (0): Linear(in_features=1, out_features=64, bias=True)
      (1): Linear(in_features=1, out_features=64, bias=True)
    )
    (1): ModuleList(
      (0): Linear(in_features=1, out_features=64, bias=True)
      (1): Linear(in_features=1, out_features=64, bias=True)
    )
  )
  (filter_hidden): ModuleList(
    (0): ModuleList(
      (0): ModuleList(
        (0): Linear(in_features=64, out_features=66, bias=True)
        (1): Linear(in_features=66, out_features=32, bias=True)
      )
      (1): ModuleList(
        (0): Linear(in_features=64, out_features=66, bias=True)
        (1): Linear(in_features=66, out_features=32, bias=True)
      )
    )
    (1): ModuleList(
      (0): ModuleList(
        (0): Linear(in_features=64, out_features=66, bias=True)
        (1): Linear(in_features=66, out_features=32, bias=True)
      )
      (1): ModuleList(
        (0): Linear(in_features=64, out_features=66, bias=True)
        (1): Linear(in_features=66, out_features=32, bias=True)
      )
    )
  )
  (fitting_input): ModuleList(
    (0): Linear(in_features=160, out_features=297, bias=True)
    (1): Linear(in_features=160, out_features=297, bias=True)
  )
  (fitting_hidden): ModuleList(
    (0): ModuleList(
      (0): Linear(in_features=297, out_features=238, bias=True)
      (1): Linear(in_features=238, out_features=125, bias=True)
      (2): Linear(in_features=125, out_features=40, bias=True)
    )
    (1): ModuleList(
      (0): Linear(in_features=297, out_features=238, bias=True)
      (1): Linear(in_features=238, out_features=125, bias=True)
      (2): Linear(in_features=125, out_features=40, bias=True)
    )
  )
  (fitting_out): ModuleList(
    (0): Linear(in_features=40, out_features=1, bias=True)
    (1): Linear(in_features=40, out_features=1, bias=True)
  )
)
Number of parameters in the net: 333736
Start training using device:  cuda , count:  1
LR update: lr = 0.000500
Force check:
 tensor([[ 1.2785951608e-03, -4.8172628237e-04, -2.5608170281e-03,
          1.0423494988e-03,  3.6678515570e-03, -9.8931995149e-04,
          1.2599455526e-03,  1.0267091169e-03,  3.9450311924e-03,
         -5.0656241891e-03,  3.0826341816e-03,  1.6351994685e-03,
          6.7856135335e-05,  9.8235828796e-04, -1.9799126780e-03,
         -2.7750823225e-03, -3.3900015445e-03,  5.4128604163e-03,
         -1.2815569954e-03,  1.1053250685e-03,  2.7331885935e-03,
          3.1118457017e-03,  3.9060545277e-03, -5.3394201084e-03,
          5.0105820017e-04, -3.4848140329e-03, -6.6948823306e-04,
         -2.5661285973e-03, -9.8014380080e-04,  2.2670197191e-04,
          2.2489736847e-03, -5.7908137797e-03,  2.4881874632e-03,
          1.9099657278e-03,  2.1127179007e-04, -2.2084766944e-05,
          1.0533477026e-03, -4.9984184297e-03, -6.2584256350e-03,
         -1.7196305570e-03, -1.7671630110e-03,  1.7395086680e-03,
         -2.0306198826e-03, -1.4632071739e-03, -9.1307621288e-04,
         -8.5831947797e-04,  3.3232422620e-03, -1.6790616131e-03,
          4.5622623750e-03,  1.3323027549e-03, -2.3433198630e-04,
          8.2164124561e-05,  2.4271955139e-03, -5.3392739146e-03,
          3.2365868830e-03, -8.3781665095e-04, -1.0642403413e-04,
         -2.9770212505e-05, -6.0415790296e-04, -2.4911809733e-03,
          2.1275557375e-03, -1.7836257955e-03,  7.9299618910e-03,
         -3.1065540824e-04, -1.8158863817e-03, -1.0374836342e-03,
          2.5534336023e-03,  7.8272128700e-04,  3.2144202003e-03,
          2.2739143821e-03,  1.3864543949e-03,  2.3078229407e-03,
         -1.3219009026e-03,  2.0422678261e-03, -1.5953797706e-03,
          2.9982571631e-03, -1.1508342909e-03,  9.8362528065e-04,
         -7.7839972488e-04, -2.0600358254e-03,  1.2259267311e-03,
          5.5757853932e-04,  1.8874455931e-03,  3.5733917886e-04,
          1.1132973124e-03,  2.8041540304e-03,  3.8321788876e-03,
         -2.7638970472e-03,  2.4461664288e-03,  3.4502297794e-03,
         -1.9614523985e-03,  2.5150888537e-03,  1.0504831559e-04,
         -2.3083458628e-03,  1.3917699804e-03,  6.8379303647e-04,
          2.1566082884e-03,  1.9571274919e-03, -2.2425209469e-03,
         -8.6494107753e-04,  1.9834579931e-03, -1.1150473468e-03,
         -5.4766757729e-04, -8.1387591753e-04, -2.7840867910e-03,
         -2.1074939223e-03, -3.9878019411e-03, -1.1293352922e-03,
          4.5854869702e-04, -1.2260048816e-03,  3.6262269136e-03,
          1.4787633617e-03,  8.5838220957e-04,  2.2236090740e-03,
         -1.7406916626e-03,  2.6967255483e-04,  1.9584005485e-05,
          2.4551583899e-04,  4.5862656040e-03, -3.1852146063e-03,
         -3.7021642323e-03, -1.4278671688e-03, -2.3214972325e-03,
          1.3661330624e-03, -1.8099108539e-03, -5.2474228212e-03,
         -1.9583788673e-03,  1.6990760441e-03, -2.2247610910e-03,
         -1.4325328309e-03, -1.2578316840e-03,  6.2061511295e-03,
         -9.9735900648e-04, -3.5421761142e-03, -4.7143622936e-03,
          1.4483769299e-03, -4.0687752799e-03,  1.6693244166e-03,
         -1.0320906785e-05,  1.0678933911e-03,  1.6400940673e-04],
        [ 3.1849414036e-04, -2.5466624446e-03,  5.8469532171e-03,
          2.7471330874e-03, -1.5183827854e-03, -2.3199004697e-03,
         -1.5147834968e-03,  1.7944210658e-04,  7.1048776391e-04,
         -4.1394500019e-03, -7.5679280641e-04,  4.1494009883e-03,
         -5.6814890066e-05,  5.4321849094e-03,  1.1392871764e-03,
         -3.2760328000e-03,  1.6070306915e-03,  3.8117969585e-03,
         -2.9326482611e-03,  1.9271042005e-03,  1.1399257734e-04,
          2.4834927492e-03, -4.3872950612e-03, -1.6381869880e-03,
          1.1726352792e-03, -1.5366161269e-03, -2.7308326083e-03,
          7.7317831232e-04,  1.2100994803e-03,  1.8288742163e-03,
         -6.1074203169e-04, -1.6037083922e-04, -4.4116935851e-05,
         -4.9574313842e-05, -2.8351648332e-04, -4.2650674110e-03,
          6.4866629892e-04, -1.6830648345e-03, -5.3888057938e-03,
          2.4799249380e-03,  2.3389810171e-04, -4.0359484635e-04,
         -4.0097079614e-04,  2.0263628054e-03, -1.0489775389e-04,
         -2.9697675519e-04,  1.7465950737e-03, -5.4485548705e-03,
         -7.0781796882e-04, -3.0915780263e-03, -7.5230302013e-03,
          2.4672638850e-04, -1.8204181798e-03,  5.5761675030e-03,
          1.1582032183e-03,  4.3149731508e-03,  1.2829917217e-03,
          2.4835306055e-03, -3.2515286550e-03, -4.7908996266e-03,
         -2.4854473244e-04, -1.7819079103e-03, -4.0595090246e-03,
         -5.4420137767e-03,  2.6479882753e-03, -8.3522255458e-05,
          3.5901261110e-03, -9.5036303313e-05,  2.9885167133e-03,
         -3.3000312920e-03, -1.1983261113e-04, -1.7815856622e-03,
         -1.8416666629e-03, -4.4924754224e-03,  8.3803647326e-03,
          2.2001050662e-03, -1.6153596308e-03,  4.4546574843e-03,
         -2.5356998178e-03,  1.2297810724e-03,  1.4612535510e-03,
          6.1815198344e-03, -6.9619553186e-04,  2.9665862320e-03,
          1.6593867629e-03,  8.1946100026e-04,  3.3882871103e-03,
         -2.1972379617e-03,  8.2229506364e-05, -7.0764070427e-04,
         -4.0129092615e-03, -1.8308861368e-03,  1.0330753978e-03,
         -3.3914445377e-03,  5.3533776052e-03,  4.7100540070e-03,
          4.2981870722e-04, -1.6503778608e-03, -2.8723143082e-04,
         -5.0249538731e-04,  9.5275651940e-04,  1.1083219460e-03,
         -1.1571933500e-03, -4.0876081866e-04, -2.7687226520e-03,
         -1.4311434148e-03, -6.0808283457e-04, -4.6780379828e-03,
         -4.1619043140e-04,  4.0705380865e-03,  2.2060162389e-04,
          4.0428387357e-03, -2.6193075364e-03,  3.1673410067e-04,
          9.8873268421e-04,  2.3394452391e-03, -5.2292181472e-03,
          6.4241233616e-03,  3.5157458377e-04,  3.7239683032e-03,
         -1.5481157398e-03,  9.0079145824e-05,  3.6348988112e-03,
         -3.6885680549e-03, -4.4873620982e-03, -8.3628694237e-04,
          3.5474306574e-04,  1.8386699549e-03,  5.3650957003e-03,
          2.6923067461e-03,  1.3466434300e-03, -4.9725906850e-03,
         -1.7566769695e-04,  1.8186049523e-03, -2.1663777728e-03,
          1.8932333203e-03, -1.8092778571e-03,  3.9276936732e-04,
          9.0581402030e-04,  1.6322489036e-03, -6.3765264382e-03],
        [-1.5908483357e-03, -2.1189547773e-03,  5.3579062020e-03,
          4.1951606495e-03,  5.5587366035e-04,  4.6313246069e-03,
         -1.5846250198e-03,  4.3336127518e-04,  1.2809482863e-04,
          2.4884848126e-03,  5.0771741860e-04, -1.7794371110e-03,
         -1.3630614115e-03,  3.6196705624e-03, -9.3586057360e-04,
         -2.2566606724e-03, -9.7103663305e-04,  3.9237991850e-04,
          1.2432257326e-03, -2.2635952197e-04, -6.8703317608e-03,
          1.5243432970e-03, -1.4013765031e-03, -2.8154095936e-03,
         -1.4632013357e-03, -3.9714016788e-04,  3.0969390739e-03,
          1.6143430116e-03, -6.5398383341e-04, -2.9806560689e-03,
         -3.1204656848e-03,  2.6734760832e-03,  2.9151957989e-03,
          7.4233560542e-04, -1.1762438619e-03, -5.5721536926e-04,
         -3.6410798348e-03, -2.6418042406e-03, -4.2569462767e-04,
          4.1784752798e-03,  3.1691630176e-03,  1.7374562042e-03,
          1.8489716425e-03, -1.8277465901e-03, -2.6848876507e-03,
         -3.0558492680e-04,  3.5244909439e-03,  8.8153846826e-04,
          4.3974008868e-03,  3.7887093906e-03,  5.1436028814e-04,
          4.0650728333e-03,  5.8211560177e-04, -7.9529622395e-04,
         -1.6386523943e-03, -1.4564185963e-03, -1.7909177178e-03,
         -6.0765407881e-04,  1.9642456652e-03, -7.1598195715e-04,
         -7.5157159860e-04,  8.0543822700e-04,  4.0892706849e-03,
         -2.7387375309e-03,  3.1408408140e-03, -1.3609854534e-03,
          8.0123374341e-05,  6.7830980576e-05,  2.7969150893e-04,
         -1.6777967316e-03,  8.6238323740e-04, -3.2591621875e-03,
          1.2504678636e-03, -1.0118746763e-03,  2.1565464084e-03,
          1.6328045788e-03, -1.8348658800e-04, -1.8409761799e-04,
         -6.0158223772e-04, -1.9910694121e-03, -1.0580054063e-06,
         -1.2326939431e-03,  5.1853227253e-03, -2.3230774019e-05,
         -2.7431885907e-03,  2.7253141784e-03,  1.5011328148e-03,
          1.2792865521e-03,  1.1363947076e-03, -2.4614778376e-03,
         -1.7497065804e-03, -1.4264286998e-03, -4.0620245979e-03,
         -1.0820780329e-03, -1.1435371309e-03, -4.0158304508e-03,
          3.8377278472e-03, -1.0868870793e-03, -3.7437485713e-03,
         -7.6290309546e-04, -3.1935105535e-03,  4.7096971578e-03,
         -3.7274708599e-03, -6.1015009339e-04, -9.2587442588e-04,
         -6.1682086120e-04, -1.1298522967e-03,  1.6488214945e-03,
          1.2755781794e-03, -1.6091994924e-03, -7.1047477191e-04,
          5.1255857579e-04,  1.6485640958e-03, -1.1574424870e-03,
          1.0863354535e-04, -6.2744222063e-04,  5.7833394541e-04,
          7.1536589583e-04,  2.3248395702e-04,  2.9159773214e-03,
          6.0126201948e-04, -3.5635147080e-03,  2.2399332928e-03,
         -2.5482380180e-03, -2.0240743250e-03,  2.0964301835e-04,
          1.9504773673e-03, -4.6124345643e-03,  7.5168531798e-04,
          2.2433934859e-04, -4.6913479870e-04, -2.8083444297e-03,
          2.2885202715e-03, -1.7328478514e-04,  2.4455544653e-03,
         -1.0455502037e-03, -2.4969218291e-03,  6.1629853990e-03,
         -3.2047871918e-03,  3.6004714369e-03, -2.2790279543e-03],
        [ 2.0504122513e-03,  2.1209219110e-03, -4.0715370718e-03,
          3.8114463697e-03, -2.6862718357e-03, -3.9944024537e-04,
         -7.2522525278e-03, -4.3427019081e-03,  5.2901854239e-04,
          2.1006355810e-03, -6.0505541229e-04,  4.9288212395e-03,
         -2.3984175596e-03, -5.1955045815e-03,  2.7248052463e-03,
          1.5359874712e-04,  5.0448533799e-03, -2.1169937849e-03,
         -5.9407163095e-04,  7.1864520309e-04, -4.9426241846e-03,
          6.7377598625e-04,  3.8073560328e-03, -1.1590861406e-03,
         -2.6295036312e-03, -9.6830721606e-04,  4.8768184876e-03,
          9.0498837588e-04, -9.7232900443e-05, -1.0776216750e-04,
         -3.8087962369e-03, -2.9425056478e-03, -2.5688670244e-03,
          2.4252483886e-03, -1.3584677189e-03, -4.9080151815e-03,
         -4.2828202470e-03, -1.1485057597e-03, -1.5920722158e-03,
         -6.3760924892e-04, -1.8906961979e-03,  2.3285086964e-03,
          4.0661585074e-04,  3.9910168916e-03, -3.9010125815e-04,
          2.9108911758e-03,  1.9209981043e-03, -1.6771844625e-03,
         -3.9040076340e-04,  2.1434779269e-03, -2.9379909269e-03,
         -2.2795174450e-03,  1.9611469412e-03, -1.4357953706e-03,
          1.6050207327e-03,  3.1678511872e-03,  4.7915166158e-03,
          2.2397225296e-03,  2.2610412646e-03,  1.5895875293e-03,
          2.0688745067e-03,  1.1779165546e-03, -1.5722118796e-03,
          5.1482574695e-04,  7.7535239209e-04, -2.0905153371e-03,
          1.1994841124e-03, -2.8192890838e-04, -1.4113085753e-03,
         -4.6597938363e-04, -9.0495486664e-04, -1.9150839457e-03,
          4.8360928053e-04, -9.8188512539e-04, -1.7695716551e-03,
         -2.8530731728e-03,  4.4950803523e-03,  4.7579300952e-03,
         -1.4806418216e-03, -2.7340969325e-03,  2.7132587023e-03,
          2.6518205390e-04,  1.5228655202e-03,  1.1627621840e-04,
         -2.6816481173e-03, -1.8034803891e-03,  4.2927050928e-03,
         -1.5098330959e-03, -4.7716640735e-03,  2.3490417224e-03,
         -1.8928271478e-03, -7.8911368000e-04, -2.8252472217e-04,
          1.7174938691e-03, -2.2462340318e-03,  3.5370842145e-03,
          3.5018506357e-03,  1.3290402014e-03,  4.7448120319e-03,
          6.3756107937e-03, -2.8626999702e-03,  2.2907596931e-03,
          3.9456732567e-03, -2.8774276703e-03, -1.2002577940e-03,
          8.5625782022e-04,  2.9732764501e-03, -4.3349052629e-03,
         -1.9116583605e-03,  2.9075937770e-03,  3.3196326601e-04,
         -1.3638205343e-03,  5.7724945659e-04,  2.7836170760e-03,
         -1.4216680391e-03, -9.6918682366e-04, -1.8807987336e-04,
          3.9114840456e-03,  2.6499009904e-03, -4.2861758268e-03,
         -7.5122770548e-04, -3.2777856366e-03,  4.6061075165e-03,
          1.5002056496e-03, -3.9844584311e-03,  3.3649801715e-03,
          3.1974835903e-04,  1.0733647969e-03, -4.9440072177e-03,
          6.9665226717e-04, -2.9292056023e-03,  2.2180014213e-03,
         -3.1553638847e-03,  6.1430416591e-03, -2.7090632200e-03,
         -1.9843468535e-03, -1.3723496970e-03, -4.4877637516e-03,
         -8.9383097870e-04,  1.2597300237e-03, -3.7667448308e-04]],
       device='cuda:0')
Additional parameters check:
 std:
 tensor([[[0.0656331476, 0.0399566198, 0.0399566198, 0.0399566198],
         [0.0728944964, 0.0455357023, 0.0455357023, 0.0455357023]]],
       device='cuda:0') 
avg:
 tensor([[[0.0219554946, 0.0000000000, 0.0000000000, 0.0000000000],
         [0.0301149107, 0.0000000000, 0.0000000000, 0.0000000000]]],
       device='cuda:0') 
use_std_avg True
Epoch: 0         , Batch: 0         , lossE:   3.132139 eV/atom, lossF:   0.209378 eV/A, time:      0.096 s
Rank 0: Model saved to ./freeze_model.pytorch
Training complete. Time elapsed:      0.136 s

