cuDNN version:  7401
Number of GPUs:  1
Using /tmp/torch_extensions as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /tmp/torch_extensions/test_from_cpp/build.ninja...
Building extension module test_from_cpp...
ninja: no work to do.
Loading extension module test_from_cpp...
All parameters:
>>> cutoff_1: 7.700000e+00
>>> cutoff_2: 8.000000e+00
>>> cutoff_3: 0.000000e+00
>>> cutoff_max: 8.000000e+00
>>> N_types_all_frame:  2
>>> type_index_all_frame: [15, 79]
>>> N_Atoms_max: 133
>>> SEL_A_max: 532
>>> Nframes_tot:  1
>>> sym_coord_type:  1
>>> N_sym_coord: 2128
>>> batch_size:  1
>>> stop_epoch:  1
>>> num_filter_layer:  3
>>> filter_neuron: [29, 47, 104]
>>> axis_neuron:  3
>>> num_fitting_layer:  4
>>> fitting_neuron: [245, 201, 84, 70]
>>> start_lr: 5.000000e-04
>>> (Not used)decay_steps: -1
>>> decay_epoch:  1
>>> decay_rate: 9.500000e-01
>>> start_pref_e: 1.000000e+00
>>> limit_pref_e: 1.000000e+00
>>> start_pref_f: 1.000000e+04
>>> limit_pref_f: 1.000000e+00
>>> check_step: 1000
>>> output_epoch: 10
>>> save_epoch: 10

Total number of frames:  1
Number of atoms aligned:  133
COORD_Reshape: shape =  (1, 399)
FORCE_Reshape: shape =  (1, 399)
TYPE_Reshape: shape =  (1, 133)
NEI_IDX_Reshape: shape =  (1, 70756)
NEI_COORD_Reshape: shape =  (1, 212268)
NEI_TYPE_Reshape: shape =  (70756,)
Data pre-processing complete. Building net work.

one_batch_net(
  (batch_norm): BatchNorm1d(133, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (filter_input): ModuleList(
    (0): ModuleList(
      (0): Linear(in_features=1, out_features=29, bias=True)
      (1): Linear(in_features=1, out_features=29, bias=True)
    )
    (1): ModuleList(
      (0): Linear(in_features=1, out_features=29, bias=True)
      (1): Linear(in_features=1, out_features=29, bias=True)
    )
  )
  (filter_hidden): ModuleList(
    (0): ModuleList(
      (0): ModuleList(
        (0): Linear(in_features=29, out_features=47, bias=True)
        (1): Linear(in_features=47, out_features=104, bias=True)
      )
      (1): ModuleList(
        (0): Linear(in_features=29, out_features=47, bias=True)
        (1): Linear(in_features=47, out_features=104, bias=True)
      )
    )
    (1): ModuleList(
      (0): ModuleList(
        (0): Linear(in_features=29, out_features=47, bias=True)
        (1): Linear(in_features=47, out_features=104, bias=True)
      )
      (1): ModuleList(
        (0): Linear(in_features=29, out_features=47, bias=True)
        (1): Linear(in_features=47, out_features=104, bias=True)
      )
    )
  )
  (fitting_input): ModuleList(
    (0): Linear(in_features=312, out_features=245, bias=True)
    (1): Linear(in_features=312, out_features=245, bias=True)
  )
  (fitting_hidden): ModuleList(
    (0): ModuleList(
      (0): Linear(in_features=245, out_features=201, bias=True)
      (1): Linear(in_features=201, out_features=84, bias=True)
      (2): Linear(in_features=84, out_features=70, bias=True)
    )
    (1): ModuleList(
      (0): Linear(in_features=245, out_features=201, bias=True)
      (1): Linear(in_features=201, out_features=84, bias=True)
      (2): Linear(in_features=84, out_features=70, bias=True)
    )
  )
  (fitting_out): ModuleList(
    (0): Linear(in_features=70, out_features=1, bias=True)
    (1): Linear(in_features=70, out_features=1, bias=True)
  )
)
Number of parameters in the net: 324346
Start training using device:  cuda , count:  1
LR update: lr = 0.000500
Force check:
 tensor([[-6.2017231839e-04,  7.6592750646e-05, -1.2199000262e-03,
          1.5537328768e-04,  9.9317576674e-04, -2.1870044479e-03,
          4.4191818860e-04, -7.8598658705e-04, -2.4688605938e-03,
          3.1404818451e-04,  1.7395094416e-03, -2.2665187010e-03,
          1.0133339351e-03, -8.8890525594e-04, -3.7734768601e-03,
          1.8288785786e-04, -1.0857529762e-03,  8.9020949642e-06,
          1.5742638003e-03,  2.2576018949e-03, -1.0564307009e-03,
          1.5337684256e-04, -2.5377805974e-03, -2.2114420467e-03,
         -6.1717738488e-04,  9.1878660940e-04, -1.4345541745e-03,
          1.3198330468e-03,  9.0941381606e-04, -1.3581249373e-03,
          1.8875416821e-03,  1.2178690161e-03, -6.7651845205e-04,
          1.4607375409e-03, -1.5545964093e-03, -2.5811293739e-03,
         -1.2661253104e-05,  6.0803350929e-05, -2.9561590995e-03,
          1.4092544745e-04, -2.4600732438e-04, -2.2428481257e-03,
         -1.2136069731e-03, -7.2722084712e-04, -2.0853559128e-03,
          5.5378196232e-04, -5.6109014825e-04, -8.4376476403e-04,
         -1.0774700128e-03, -1.2806909903e-04, -2.4815467851e-03,
         -7.5890859891e-04,  1.5696523078e-03, -1.7581365689e-03,
          1.7483441795e-03,  8.4094728555e-05, -2.1894963742e-03,
          2.0274716950e-03,  1.2306605556e-03, -3.0004405002e-03,
         -8.4939794912e-04, -8.7883810238e-04, -2.1589107462e-04,
          8.9499735426e-04,  4.3819461343e-04, -2.2258445914e-03,
         -8.8707444908e-04, -2.5630165827e-03, -1.5335529378e-03,
         -1.6464312117e-03, -2.8674831734e-03, -1.7266309606e-03,
          3.4218692205e-04,  2.4863190215e-03, -1.7995314889e-03,
          4.1593233068e-04, -1.9269742847e-03, -1.2933512228e-03,
          8.9168420560e-04,  1.5813101639e-03, -2.3625992950e-03,
          8.0457818095e-04, -8.8346676717e-04,  4.1040007458e-03,
          1.2188268994e-03,  2.4882804622e-04,  3.1854922230e-03,
          7.6901191086e-04, -1.1115234352e-03,  2.6889559869e-03,
         -1.6809151364e-03,  8.0447778879e-04,  3.5369155792e-03,
          7.1118918046e-05, -8.3303802978e-04,  4.5921333198e-03,
          8.0866344158e-04,  5.8591024990e-04,  2.8978563584e-03,
         -5.7483094479e-04,  8.3916956284e-04,  2.4154902603e-03,
          6.5292130190e-04,  1.4861336499e-03,  1.3092061428e-03,
          2.0990673287e-04,  3.2369650636e-03,  3.6304436448e-03,
          5.5812277278e-04,  8.8396277823e-04,  2.0943421054e-03,
          3.6042525962e-04,  5.4961467476e-04,  2.7187228813e-03,
         -6.3516844276e-04,  3.0672448907e-05,  2.0752542464e-03,
         -9.2615150560e-05,  2.4060838403e-03,  1.4833308418e-03,
          3.7091097475e-04,  1.7509577109e-04,  2.2333804001e-03,
         -1.9957436645e-06,  2.5024652523e-03,  3.4050577973e-03,
         -1.3262385411e-03,  1.5551080837e-03,  2.4714714873e-03,
          2.7296608333e-04, -6.7411884451e-04,  2.4058972478e-03,
          1.5569390010e-03, -1.0530540016e-03,  2.0638737709e-03,
         -1.8540202994e-04,  2.5150743275e-03,  1.3172323720e-03,
         -7.4436710294e-04,  1.3617948131e-03,  3.6592445028e-03,
          5.8957326821e-04,  1.5234308514e-03,  5.2252770429e-03,
         -3.4982185878e-05,  2.5490285414e-03,  1.9914124509e-03,
          4.4104154962e-04,  8.9082274678e-04,  1.7792715911e-03,
         -4.4518950201e-04,  1.7001378151e-03,  1.8926869172e-03,
         -2.4838416674e-04, -6.7587528341e-04,  3.4199087688e-03,
         -4.9115409345e-04, -1.1821602694e-03, -2.4875780419e-03,
          4.9500144151e-06, -1.0063607903e-03, -2.6521110697e-04,
          2.0117535132e-04, -3.1810484432e-03,  1.6501019615e-03,
          1.8319551812e-03, -1.6322663796e-03, -2.7190791435e-03,
          2.8506164817e-03,  1.1010597406e-03, -2.4243984360e-03,
         -8.9727109295e-04, -1.9861861906e-03,  1.3616708592e-03,
          6.5889446983e-04, -1.0859250082e-03, -3.3033985717e-04,
         -1.0428408626e-04, -8.5954923370e-04,  9.1522634002e-04,
         -1.0122163419e-03,  8.3889449121e-04, -3.6593548166e-04,
         -7.5688926557e-04,  2.6937898487e-03, -1.9173596986e-05,
         -7.2242007377e-04, -1.4139740636e-03, -3.1009040963e-03,
          2.9499691856e-03, -2.8019112381e-03,  2.7922489666e-03,
         -1.8915448448e-03,  1.7906582083e-03, -2.0941235937e-03,
          3.4598171509e-04, -4.4894309978e-04,  5.8739055996e-04,
         -1.9273659360e-04,  1.0921454148e-03,  2.4193560431e-03,
          2.7090648711e-04, -3.2113973172e-04, -2.4674421744e-03,
         -9.5510886540e-05,  1.9514565480e-04,  1.8208774045e-03,
          6.1758016149e-04,  1.2641660097e-03,  1.7739088932e-03,
         -2.1354823349e-03, -6.9782294399e-04, -1.1637987422e-03,
          1.2897678649e-03, -1.0078227898e-03, -1.2816005641e-03,
         -8.9436452847e-05, -8.9700812515e-04,  9.5223217895e-04,
         -3.7732934567e-05,  9.3800911353e-04, -5.9085377513e-04,
          2.2887873155e-04,  2.5735088244e-03,  1.2804542936e-04,
         -5.4496728611e-04, -2.3645718688e-03,  5.6071828331e-04,
         -1.7091350030e-03, -1.3214431124e-03,  2.8193545098e-05,
          5.0932033529e-04,  2.8701612655e-03,  9.4735277336e-04,
          1.6036157306e-04, -3.2546356650e-03,  1.8157982714e-03,
         -7.3142032030e-04,  7.1725456923e-04, -2.7787873104e-03,
          2.8430501061e-04, -4.8617761978e-04,  1.1661378891e-03,
         -8.5813773495e-05,  2.0593209818e-03,  1.2505787080e-03,
         -1.4183663528e-04, -4.3066383011e-04, -1.0010087361e-03,
         -6.9141627964e-04, -5.0571405029e-04, -2.1507451690e-03,
          4.7166294415e-05,  2.8530087635e-03,  2.2876206379e-03,
          1.1866359860e-03,  3.9009062172e-04,  4.0491992400e-04,
         -6.5939313170e-04,  3.0785853129e-03, -9.2399404297e-04,
          7.3259461248e-04,  4.8252114926e-04,  7.0967354105e-04,
         -1.3997719688e-03, -1.8895747961e-03,  5.1573904408e-04,
         -4.7709657064e-04, -5.0592972897e-04,  4.5602973058e-04,
         -4.8651512529e-04, -5.7237788184e-04,  7.4158942482e-04,
          7.6201084582e-04,  1.9447936393e-03, -3.7322668545e-04,
         -9.6320455409e-04,  4.1944440997e-04, -7.8229021581e-04,
         -5.1328369556e-04, -2.3945855101e-03,  5.9490863413e-04,
         -1.8103530866e-03, -2.7232314633e-05, -2.2168743810e-03,
          7.7245901304e-04, -3.1355872809e-04, -2.0957724180e-03,
         -9.3560833252e-04,  2.8706022445e-03,  1.2543530804e-04,
          8.1883902836e-04, -1.3390429221e-03, -2.1556063054e-03,
          1.3668286785e-03, -1.5986291033e-03,  6.9824422253e-04,
          6.9372025282e-04, -2.6719269595e-03,  3.5211997607e-06,
         -1.2530229798e-03, -3.0830641706e-04, -2.1192448975e-03,
         -1.3387969509e-04, -2.5068770326e-04, -6.0238364458e-04,
         -5.4237060427e-04, -1.7707017220e-03,  9.0031388512e-04,
         -1.2479836502e-03,  3.5042894975e-03, -1.2093752583e-03,
         -6.4972411835e-04,  6.3275252337e-04,  1.2468865584e-03,
          1.2132366393e-03, -1.4904793151e-03,  1.9299592713e-03,
          4.4211918234e-04,  3.1274092159e-03, -1.0853116808e-03,
         -2.2132993268e-03,  5.8302754221e-04,  9.2205459994e-05,
          5.5070225678e-04, -1.9620565780e-03, -1.9895927376e-04,
          2.0856626755e-03,  1.1210116273e-03, -1.9473781129e-03,
          2.5491690527e-04,  3.9601985744e-04,  1.0143694513e-03,
         -4.5836270047e-04, -1.3456671678e-03,  1.9945259278e-03,
          2.1601780445e-03,  4.4315346436e-04, -4.5576940271e-04,
         -9.1568519395e-04,  1.7381844134e-03, -1.9361847927e-04,
         -3.4578308658e-04, -9.6726367380e-04,  1.6097390899e-03,
         -7.8869014897e-04, -5.9361755967e-04, -5.4322451590e-04,
          8.3470703985e-04, -7.4454571308e-04, -1.9845168258e-03,
          1.6653519164e-03, -3.2361732238e-03,  1.3621739823e-03,
         -1.0227726178e-03,  7.3853669664e-04, -7.2763422052e-04,
         -5.6677410559e-04,  5.2404310776e-04, -3.3740889605e-04,
          1.8899717244e-05, -4.5738762256e-03,  1.5017214675e-03,
         -1.6763281379e-03, -2.3663965666e-03, -6.1920240239e-04,
         -1.8944177428e-03,  1.1580503939e-03, -6.3236660513e-04,
         -6.6008350200e-04,  5.9201842661e-05,  2.1122680115e-03,
         -7.1221607254e-05,  2.7382970779e-04, -1.6990747674e-03,
         -3.0991628502e-04, -1.3042957273e-03, -1.6200321662e-04,
         -6.0454171559e-04, -3.2273802036e-03,  1.9204228718e-03,
          4.2932331859e-04,  6.3517381551e-04, -2.4880372645e-03,
         -1.6791189007e-03, -1.6361998961e-03, -1.7291653430e-03,
         -9.1089815371e-04,  1.0677631261e-03, -2.6294455956e-03,
          2.3679771274e-04, -6.2429514647e-04, -1.3780599525e-03,
         -1.8337742953e-03, -7.0860956426e-04, -2.8806737318e-03,
          3.2767498893e-04, -2.9283488732e-04, -1.6371585533e-03]],
       device='cuda:0')
Additional parameters check:
 std:
 tensor([[[0.0682482702, 0.0417576421, 0.0417576421, 0.0417576421],
         [0.0746984462, 0.0469723052, 0.0469723052, 0.0469723052]]],
       device='cuda:0') 
avg:
 tensor([[[0.0243756083, 0.0000000000, 0.0000000000, 0.0000000000],
         [0.0322597296, 0.0000000000, 0.0000000000, 0.0000000000]]],
       device='cuda:0') 
use_std_avg True
Epoch: 0         , Batch: 0         , lossE:   3.044688 eV/atom, lossF:   0.182237 eV/A, time:      0.075 s
Rank 0: Model saved to ./freeze_model.pytorch
Training complete. Time elapsed:      0.110 s

