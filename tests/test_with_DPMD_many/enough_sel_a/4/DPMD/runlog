2019-07-12 22:39:04.300540: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-12 22:39:04.333952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-12 22:39:04.334202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: GeForce GTX TITAN major: 3 minor: 5 memoryClockRate(GHz): 0.98
pciBusID: 0000:01:00.0
totalMemory: 5.94GiB freeMemory: 5.43GiB
2019-07-12 22:39:04.334215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2019-07-12 22:39:04.511500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-12 22:39:04.511541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2019-07-12 22:39:04.511547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2019-07-12 22:39:04.511622: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5219 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:01:00.0, compute capability: 3.5)
# DEEPMD:  _____               _____   __  __  _____           _     _  _   
# DEEPMD: |  __ \             |  __ \ |  \/  ||  __ \         | |   (_)| |  
# DEEPMD: | |  | |  ___   ___ | |__) || \  / || |  | | ______ | | __ _ | |_ 
# DEEPMD: | |  | | / _ \ / _ \|  ___/ | |\/| || |  | ||______|| |/ /| || __|
# DEEPMD: | |__| ||  __/|  __/| |     | |  | || |__| |        |   < | || |_ 
# DEEPMD: |_____/  \___| \___||_|     |_|  |_||_____/         |_|\_\|_| \__|
# DEEPMD: 
# DEEPMD: Please read and cite:
# DEEPMD: Wang, Zhang, Han and E, Comput.Phys.Comm. 228, 178-184 (2018)
# DEEPMD: 
# DEEPMD: ---Summary of the training---------------------------------------
# DEEPMD: installed to:       /home/aurora/Softwares/deepmd
# DEEPMD: source :            v0.12.1
# DEEPMD: source brach:       master
# DEEPMD: source commit:      5caff7d
# DEEPMD: source commit at:   2018-12-06 11:03:13 +0800
# DEEPMD: build float prec:   double
# DEEPMD: build with tf inc:  /home/aurora/Softwares/tensorflow-1.8/include
# DEEPMD: build with tf lib:  /home/aurora/Softwares/tensorflow-1.8/lib/libtensorflow_cc.so
# DEEPMD:                     /home/aurora/Softwares/tensorflow-1.8/lib/libtensorflow_framework.so
# DEEPMD: running on:         aurora-System-Product-Name
# DEEPMD: gpu per node:       None
# DEEPMD: num_inter_threads:  1
# DEEPMD: num_intra_threads:  1
# DEEPMD: -----------------------------------------------------------------
# DEEPMD: 
# DEEPMD: ---Summary of DataSystem-----------------------------------------
# DEEPMD: find 1 system(s):
# DEEPMD:                                     system  natoms  bch_sz  n_bch
# DEEPMD:                                   systems/      29       2      1
# DEEPMD: -----------------------------------------------------------------
# DEEPMD: 
2019-07-12 22:39:04.521523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2019-07-12 22:39:04.521554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-12 22:39:04.521560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2019-07-12 22:39:04.521565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2019-07-12 22:39:04.521607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5219 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:01:00.0, compute capability: 3.5)
mesh: [array([0, 0, 0, 2, 2, 2], dtype=int32)]
descrpt_deriv.shape: (2, 201840)
d_all: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]] (2, 67280)
Start, end 0 0
dd: []
deriv_debug_cur_type: []
ddr: []
dda: []
from inside:sumr, suma, sumn, sumr2, suma2 0.0 0.0 0 0.0 0.0
Start, end 0 67280
dd: [[0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]]
deriv_debug_cur_type: [[0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]]
ddr: [[0.]
 [0.]
 [0.]
 ...
 [0.]
 [0.]
 [0.]]
dda: [[0. 0. 0.]
 [0. 0. 0.]
 [0. 0. 0.]
 ...
 [0. 0. 0.]
 [0. 0. 0.]
 [0. 0. 0.]]
from inside:sumr, suma, sumn, sumr2, suma2 843.1803851410652 -2.6645352591003757e-15 33640 176.5425572303489 58.84751907678296
sumr, suma, sumn, sumr2, suma2 first: [[0.0, 843.1803851410652]] [[0.0, -2.6645352591003757e-15]] [[0, 33640]] [[0.0, 176.5425572303489]] [[0.0, 58.84751907678296]]
sumr, suma, sumn, sumr2, suma2: [  0.         843.18038514] [ 0.00000000e+00 -2.66453526e-15] [    0 33640] [  0.         176.54255723] [ 0.         58.84751908]
ntypes: 2
dstdunit: [nan, nan, nan, nan]
davgunit: [nan, 0, 0, 0]
ntypes: 2
dstdunit: [0.06796874427053519, 0.0418250130959348, 0.0418250130959348, 0.0418250130959348]
davgunit: [0.025064815253896113, 0, 0, 0]
# DEEPMD: computed coord/descrpt stats
sys_tynatom [[ 0. 29.]]
sys_ener [-88.31943]
Energy_shift: [ 0.         -3.04549759]
# DEEPMD: computed energy bias
dstd: [[       nan        nan        nan ...        nan        nan        nan]
 [0.06796874 0.04182501 0.04182501 ... 0.04182501 0.04182501 0.04182501]]
# DEEPMD: built lr
DS_LAYER: w:stddev 1.0 outputs_size: 28 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 84 28 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 95 84 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 28 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 84 28 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 95 84 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 950 231 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 231 117 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 117 106 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 106 52 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 52 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 28 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 84 28 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 95 84 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 28 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 84 28 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 95 84 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 950 231 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 231 117 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 117 106 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 106 52 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 52 1 b:stddev 1.0 bavg -3.0454975862068965
bias_energy_e [ 0.         -3.04549759]
DS_LAYER: w:stddev 1.0 outputs_size: 28 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 84 28 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 95 84 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 28 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 84 28 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 95 84 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 950 231 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 231 117 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 117 106 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 106 52 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 52 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 28 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 84 28 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 95 84 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 28 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 84 28 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 95 84 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 950 231 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 231 117 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 117 106 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 106 52 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 52 1 b:stddev 1.0 bavg -3.0454975862068965
bias_energy_e [ 0.         -3.04549759]
DS_LAYER: w:stddev 1.0 outputs_size: 28 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 84 28 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 95 84 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 28 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 84 28 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 95 84 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 950 231 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 231 117 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 117 106 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 106 52 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 52 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 28 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 84 28 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 95 84 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 28 1 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 84 28 b:stddev 1.0 bavg 0.0
DS_LAYER: w:stddev 1.0 outputs_size: 95 84 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 950 231 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 231 117 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 117 106 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 106 52 b:stddev 1.0 bavg 0.0
ONE_LAYER: w:stddev 1.0 outputs_size: 52 1 b:stddev 1.0 bavg -3.0454975862068965
bias_energy_e [ 0.         -3.04549759]
dddddddddddddddddddd
0
# DEEPMD: built network
(1, 28)
(1, 28)
(28, 84)
(1, 84)
(84, 95)
(1, 95)
(1, 28)
(1, 28)
(28, 84)
(1, 84)
(84, 95)
(1, 95)
(950, 231)
(231,)
(231, 117)
(117,)
(117, 106)
(106,)
(106, 52)
(52,)
(52, 1)
(1,)
(1, 28)
(1, 28)
(28, 84)
(1, 84)
(84, 95)
(1, 95)
(1, 28)
(1, 28)
(28, 84)
(1, 84)
(84, 95)
(1, 95)
(950, 231)
(231,)
(231, 117)
(117,)
(117, 106)
(106,)
(106, 52)
(52,)
(52, 1)
(1,)
total number of trainable variables:  572168
# DEEPMD: built training
2019-07-12 22:39:06.859058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2019-07-12 22:39:06.859100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-12 22:39:06.859106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2019-07-12 22:39:06.859111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2019-07-12 22:39:06.859164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5219 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:01:00.0, compute capability: 3.5)
# DEEPMD: initialize model from scratch
# DEEPMD: start training at lr 5.00e-04 (== 5.00e-04), final lr will be 5.00e-04
Check from c std:1.000000
Check from c std:1.000000
Check from c std:-nan
force_hat_reshape: [[0.0024781477446699923 0.0010884596617509268 -0.0037558739441880267 0.011473398094286828 0.00066640337717049577 -0.0035662801810301538 0.0023895686971456159 0.011130314305072149 0.0040536307736229469 0.00685365980783661 -0.0033783698165400227 -0.0025509362596698262 0.012297579467467469 0.00652270889401796 -0.00310464190360762 0.012638028641330708 -0.00047785522628134968 -0.0091518059456383027 -0.0043860373647074359 -1.2346625507868959e-05 0.0035422545421330662 0.0086909412368913284 0.0029561738567710463 -0.003100773848378049 -0.0075138644133996456 -0.0044519605050957253 -0.00056734799971976883 0.0078165959489553891 -0.0023565968280746725][-0.0041919332077947754 -0.000646015423315195 -0.0085690212166513787 -0.0067490483043981375 0.003947573494336055 -0.0031159776063961961 -0.0048251407746156983 0.0022908027609997248 -0.0052525606870307421 0.0026561185513812534 0.0069631100646217053 0.0019143427267587678 -0.0046057960834533394 -0.00376258494119107 0.015238602911013182 0.0076969639724408841 -0.0047092087119013021 -0.001056496145042945 0.0030487151731221364 -0.014375219952996173 -0.013729910266580922 0.0085207562445386714 -0.0076270004781380492 0.0064194132489408893 -0.0048245334445158059 -0.0045755000402574846 -0.012233308602017993 -0.0018890365928479068 -0.0055474221380403237][0.00248436029848265 0.0024662072687468405 -0.010132044077819468 -0.0072374412623511346 0.0036997418960573866 -0.0080608458146795656 0.0016931224686737838 0.0081927140390053756 -0.0033894120519326588 0.00667954007929369 -0.0011227476315312788 -0.0066054513660118294 -0.0072070663614275535 0.0076938200141839176 -0.0090542869301228311 -0.0017188238072941271 0.0091266084063852931 0.010071912307858472 0.00046013337796593589 -0.0055500604725749426 0.0032785129290363985 0.0061300821204489731 -0.0011383642534961548 0.0081509318959744788 0.0021208731345384625 -0.000318676719322633 -0.0089558693868977534 0.0052932044953937661 0.00031546668516466375]]
l2_force_loss:[0.0017415127519442594]
Check from c std:-nan
force_hat_reshape: [[0.0024781477446699923 0.0010884596617509268 -0.0037558739441880267 0.011473398094286828 0.00066640337717049577 -0.0035662801810301538 0.0023895686971456159 0.011130314305072149 0.0040536307736229469 0.00685365980783661 -0.0033783698165400227 -0.0025509362596698262 0.012297579467467469 0.00652270889401796 -0.00310464190360762 0.012638028641330708 -0.00047785522628134968 -0.0091518059456383027 -0.0043860373647074359 -1.2346625507868959e-05 0.0035422545421330662 0.0086909412368913284 0.0029561738567710463 -0.003100773848378049 -0.0075138644133996456 -0.0044519605050957253 -0.00056734799971976883 0.0078165959489553891 -0.0023565968280746725 -0.0041919332077947754 -0.000646015423315195 -0.0085690212166513787 -0.0067490483043981375 0.003947573494336055 -0.0031159776063961961 -0.0048251407746156983 0.0022908027609997248 -0.0052525606870307421 0.0026561185513812534 0.0069631100646217053 0.0019143427267587678 -0.0046057960834533394 -0.00376258494119107 0.015238602911013182 0.0076969639724408841 -0.0047092087119013021 -0.001056496145042945 0.0030487151731221364 -0.014375219952996173 -0.013729910266580922 0.0085207562445386714 -0.0076270004781380492 0.0064194132489408893 -0.0048245334445158059 -0.0045755000402574846 -0.012233308602017993 -0.0018890365928479068 -0.0055474221380403237][0.00248436029848265 0.0024662072687468405 -0.010132044077819468 -0.0072374412623511346 0.0036997418960573866 -0.0080608458146795656 0.0016931224686737838 0.0081927140390053756 -0.0033894120519326588 0.00667954007929369 -0.0011227476315312788 -0.0066054513660118294 -0.0072070663614275535 0.0076938200141839176 -0.0090542869301228311 -0.0017188238072941271 0.0091266084063852931 0.010071912307858472 0.00046013337796593589 -0.0055500604725749426 0.0032785129290363985 0.0061300821204489731 -0.0011383642534961548 0.0081509318959744788 0.0021208731345384625 -0.000318676719322633 -0.0089558693868977534 0.0052932044953937661 0.00031546668516466375 0.0091392122274531942 -0.0018717447924551844 0.006319190612508999 -0.0010872750757742217 0.0012906870296764746 0.0062356745930525481 0.010129337175612063 -0.0030797854332185173 -0.012051780047235602 -0.0014224154491817736 -0.0004031947264634107 -2.9785285571428389e-05 -0.011169739502229863 0.001219867666577488 -0.000275151504123337 -0.0056838178346750095 0.0051362248150364567 0.00020091026187921569 0.0063804898809270739 -0.0012437674361499658 -0.00574453597004507 0.0044996423391098824 -0.0019732369265574785 -0.0025057574638333189 -0.010292442188658376 0.00015081662715271107 0.00050827084157851652 -0.000764426593873763 -0.00035826435086252676][0.00033271978989473516 -0.0028862189585220632 0.00057946737981453406 0.0013155027056313254 0.0067251691603143855 -0.0060740819667360717 -0.0027185323457156333 -0.010097534700785876 0.0079177660601755864 0.0028984953973660091 0.0069531058147917144 -0.00015846552012368313 0.0030785135628082494 0.0010159644180639271 0.0029823442431071152 0.0030564204421048517 -0.0079720895426833513 -0.007103245063655869 0.0054248003672778726 0.0018262160331603756 -0.0026326701042984745 0.0050648711499017372 0.0039912569705852037 -0.0028994423309718445 0.0039567871652184229 -0.0025211925221699464 0.0040657594116699156 0.00024824701446898368 -0.00086117615124248626 0.00049670270241772188 -0.00052376825953394217 -0.0013302461679267673 -0.0025385814448958717 -0.0065399608849064043 0.0099945511060289451 -0.0037346369473563678 -0.00028605503754013562 -0.0029212161397794184 0.00562864495158556 -0.00759276300025671 0.0026103639014020078 0.0027167134339414942 -0.00092330815865086585 0.00046732881603918895 -0.00031310195436986708 -0.00090671606028706 -0.00835603412722942 -0.0061040633387252838 -0.0031917262435143131 -0.0060794341743958561 -0.0038366315677500159 0.0023248323526179077 0.00960866779454291 0.0039346863587151988 -0.0029234626498044584 0.00010395349109729735 0.0082052332247201567 0.0052480666547089182]]
l2_force_loss:[0.0073685792067223295]
Check l2_l_bch from optimizer[228.63464924033011]
2019-07-12 22:39:08.369259: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcupti.so.9.0 locally
Check from c std:-nan
force_hat_reshape: [[0.0024781477446699923 0.0010884596617509268 -0.0037558739441880267 0.011473398094286828 0.00066640337717049577 -0.0035662801810301538 0.0023895686971456159 0.011130314305072149 0.0040536307736229469 0.00685365980783661 -0.0033783698165400227 -0.0025509362596698262 0.012297579467467469 0.00652270889401796 -0.00310464190360762 0.012638028641330708 -0.00047785522628134968 -0.0091518059456383027 -0.0043860373647074359 -1.2346625507868959e-05 0.0035422545421330662 0.0086909412368913284 0.0029561738567710463 -0.003100773848378049 -0.0075138644133996456 -0.0044519605050957253 -0.00056734799971976883 0.0078165959489553891 -0.0023565968280746725 -0.0041919332077947754 -0.000646015423315195 -0.0085690212166513787 -0.0067490483043981375 0.003947573494336055 -0.0031159776063961961 -0.0048251407746156983 0.0022908027609997248 -0.0052525606870307421 0.0026561185513812534 0.0069631100646217053 0.0019143427267587678 -0.0046057960834533394 -0.00376258494119107 0.015238602911013182 0.0076969639724408841 -0.0047092087119013021 -0.001056496145042945 0.0030487151731221364 -0.014375219952996173 -0.013729910266580922 0.0085207562445386714 -0.0076270004781380492 0.0064194132489408893 -0.0048245334445158059 -0.0045755000402574846 -0.012233308602017993 -0.0018890365928479068 -0.0055474221380403237][0.00248436029848265 0.0024662072687468405 -0.010132044077819468 -0.0072374412623511346 0.0036997418960573866 -0.0080608458146795656 0.0016931224686737838 0.0081927140390053756 -0.0033894120519326588 0.00667954007929369 -0.0011227476315312788 -0.0066054513660118294 -0.0072070663614275535 0.0076938200141839176 -0.0090542869301228311 -0.0017188238072941271 0.0091266084063852931 0.010071912307858472 0.00046013337796593589 -0.0055500604725749426 0.0032785129290363985 0.0061300821204489731 -0.0011383642534961548 0.0081509318959744788 0.0021208731345384625 -0.000318676719322633 -0.0089558693868977534 0.0052932044953937661 0.00031546668516466375 0.0091392122274531942 -0.0018717447924551844 0.006319190612508999 -0.0010872750757742217 0.0012906870296764746 0.0062356745930525481 0.010129337175612063 -0.0030797854332185173 -0.012051780047235602 -0.0014224154491817736 -0.0004031947264634107 -2.9785285571428389e-05 -0.011169739502229863 0.001219867666577488 -0.000275151504123337 -0.0056838178346750095 0.0051362248150364567 0.00020091026187921569 0.0063804898809270739 -0.0012437674361499658 -0.00574453597004507 0.0044996423391098824 -0.0019732369265574785 -0.0025057574638333189 -0.010292442188658376 0.00015081662715271107 0.00050827084157851652 -0.000764426593873763 -0.00035826435086252676][0.00033271978989473516 -0.0028862189585220632 0.00057946737981453406 0.0013155027056313254 0.0067251691603143855 -0.0060740819667360717 -0.0027185323457156333 -0.010097534700785876 0.0079177660601755864 0.0028984953973660091 0.0069531058147917144 -0.00015846552012368313 0.0030785135628082494 0.0010159644180639271 0.0029823442431071152 0.0030564204421048517 -0.0079720895426833513 -0.007103245063655869 0.0054248003672778726 0.0018262160331603756 -0.0026326701042984745 0.0050648711499017372 0.0039912569705852037 -0.0028994423309718445 0.0039567871652184229 -0.0025211925221699464 0.0040657594116699156 0.00024824701446898368 -0.00086117615124248626 0.00049670270241772188 -0.00052376825953394217 -0.0013302461679267673 -0.0025385814448958717 -0.0065399608849064043 0.0099945511060289451 -0.0037346369473563678 -0.00028605503754013562 -0.0029212161397794184 0.00562864495158556 -0.00759276300025671 0.0026103639014020078 0.0027167134339414942 -0.00092330815865086585 0.00046732881603918895 -0.00031310195436986708 -0.00090671606028706 -0.00835603412722942 -0.0061040633387252838 -0.0031917262435143131 -0.0060794341743958561 -0.0038366315677500159 0.0023248323526179077 0.00960866779454291 0.0039346863587151988 -0.0029234626498044584 0.00010395349109729735 0.0082052332247201567 0.0052480666547089182]]
Check from c std:-nan
force_hat_reshape: [[0.0024563638813395552 0.0010712147432016552 -0.0033683158977560316 0.01072942382503636 0.00092008536434141561 -0.0028918491449085632 0.0022165101740438992 0.009994881431863438 0.0039258147902512062 0.0070499043928935762 -0.0032525796502123287 -0.0023623848899773196 0.011373819399856412 0.0054288725273526073 -0.0023464020049089951 0.011312042539155296 -0.00018858189027132772 -0.0076642576803846209 -0.0041435241532843713 0.00015065888115355219 0.0030918114502808 0.007814755205023817 0.0023127250906157105 -0.0025501045204867029 -0.0072578272519677181 -0.0038511847702735256 -0.00036991970454412906 0.0079106655817368279 -0.0019628054510592158][-0.003577977212572313 -0.00059209710052297263 -0.0077665752411665376 -0.00588381414571511 0.0041709398084084342 -0.00296865459559094 -0.0040457987418064468 0.0021152347710426235 -0.0046976570577370947 0.002353623983302063 0.0070974187713072007 0.0012500187375684031 -0.0039947823093436745 -0.0032050230558214841 0.01383762635196841 0.0067692363663276979 -0.0044930132872821882 -0.00086155561284555573 0.0025121712698648229 -0.012746893536337489 -0.012070823991779025 0.0072175595077492811 -0.0075116812300780504 0.0063164831744212816 -0.0043482313826248009 -0.0042677710685359443 -0.011171839986079008 -0.0023331644739141857 -0.0055683645906478632][0.0026767333376550673 0.0017273820783808881 -0.0095630490869785343 -0.0067080157010007352 0.00300437697365135 -0.0074314837148326644 0.0012056949605368239 0.007129603872258518 -0.0034536119842244356 0.0065699722630987124 -0.001000985505874141 -0.0066188935729120614 -0.007420418646777332 0.0064918801242774922 -0.0088168071789246991 -0.0017662782854885152 0.00828789679729169 0.00921823334604512 0.00049170735113668231 -0.0048430512875051526 0.0030247028589168536 0.0055906833500084284 -0.0008835931599372052 0.0072323052420737244 0.0018141485049083515 -0.00017350469671733313 -0.008052278984529225 0.0050554648104505981 0.00012677954534090765]]
l2_force_loss:[0.0017402448235389707]
Check from c std:-nan
force_hat_reshape: [[0.0024563638813395552 0.0010712147432016552 -0.0033683158977560316 0.01072942382503636 0.00092008536434141561 -0.0028918491449085632 0.0022165101740438992 0.009994881431863438 0.0039258147902512062 0.0070499043928935762 -0.0032525796502123287 -0.0023623848899773196 0.011373819399856412 0.0054288725273526073 -0.0023464020049089951 0.011312042539155296 -0.00018858189027132772 -0.0076642576803846209 -0.0041435241532843713 0.00015065888115355219 0.0030918114502808 0.007814755205023817 0.0023127250906157105 -0.0025501045204867029 -0.0072578272519677181 -0.0038511847702735256 -0.00036991970454412906 0.0079106655817368279 -0.0019628054510592158 -0.003577977212572313 -0.00059209710052297263 -0.0077665752411665376 -0.00588381414571511 0.0041709398084084342 -0.00296865459559094 -0.0040457987418064468 0.0021152347710426235 -0.0046976570577370947 0.002353623983302063 0.0070974187713072007 0.0012500187375684031 -0.0039947823093436745 -0.0032050230558214841 0.01383762635196841 0.0067692363663276979 -0.0044930132872821882 -0.00086155561284555573 0.0025121712698648229 -0.012746893536337489 -0.012070823991779025 0.0072175595077492811 -0.0075116812300780504 0.0063164831744212816 -0.0043482313826248009 -0.0042677710685359443 -0.011171839986079008 -0.0023331644739141857 -0.0055683645906478632][0.0026767333376550673 0.0017273820783808881 -0.0095630490869785343 -0.0067080157010007352 0.00300437697365135 -0.0074314837148326644 0.0012056949605368239 0.007129603872258518 -0.0034536119842244356 0.0065699722630987124 -0.001000985505874141 -0.0066188935729120614 -0.007420418646777332 0.0064918801242774922 -0.0088168071789246991 -0.0017662782854885152 0.00828789679729169 0.00921823334604512 0.00049170735113668231 -0.0048430512875051526 0.0030247028589168536 0.0055906833500084284 -0.0008835931599372052 0.0072323052420737244 0.0018141485049083515 -0.00017350469671733313 -0.008052278984529225 0.0050554648104505981 0.00012677954534090765 0.00748955114687195 -0.0018775121830681538 0.0053778029061329353 -0.0010278783899027332 0.0011177098804322863 0.0054504222340479892 0.0095681683725935326 -0.0023724710208905095 -0.010586633358075167 -0.00011855561481062321 -0.00030670250219257068 -2.4834660926799363e-05 -0.0091076535017945732 0.00093829812574363464 -0.00050847467063268846 -0.0048467008918228237 0.0045581906358427111 0.00023380889203556296 0.0051921181095187473 -0.0010126280502181261 -0.0047274122437569576 0.0047512655911612937 -0.0015398915593053352 -0.0021754433822098423 -0.0093750243055618823 0.0003333987034264023 0.00064391154487790532 0.00033265253886965871 -0.00024152319425813779][0.00023659901820950003 -0.0029915644578774746 0.000506521553414537 0.0010500004560368532 0.00638983987285802 -0.0050502979151999267 -0.0022819599197986813 -0.0095540498458840072 0.0066851821754354643 0.0024511229966134159 0.0062124256303741328 -0.0003085978090272853 0.0028015893224038789 0.00071820100207660182 0.0026289125984117531 0.002831563474878855 -0.0072962780576475636 -0.0059535608917016727 0.0045738480401489295 0.0014038816352158218 -0.0022954362740492959 0.0046667545015397034 0.0032401119678863335 -0.0025635689474420836 0.003902388894925076 -0.0031084786137098694 0.0034861024178898343 0.00014329373897802855 0.00021561035985465012 0.00036405271402785986 -0.00039329523057820182 -0.0013072980395843026 -0.0022837341825900606 -0.0057684250554249413 0.00816778730659794 -0.003220042895606691 -0.00033134774904756221 -0.0026261043498167921 0.0045589840425413145 -0.0066217343142143817 0.001737248386685891 0.00213154289574915 -0.00081576394894028069 0.00039940434148474067 -0.00036580998274529457 -0.000677328566403792 -0.0077573733337267222 -0.0052965114007906188 -0.0030232603110066213 -0.0060926845098390747 -0.003258837957993518 0.0019591655458378594 0.0091328511244850065 0.0035628308645448588 -0.002637406085784139 0.00025852652544411874 0.0070754001596194625 0.0042510479301335613]]
l2_force_loss:[0.007382443921130682]
Check l2_l_bch from optimizer[221.95613625893193]
# DEEPMD: batch       1 training time 0.75 s, testing time 0.29 s
# DEEPMD: saved checkpoint model.ckpt
Start L-BFGS
0
# DEEPMD: finished training
# DEEPMD: wall time: 2.364 s
/home/aurora/PycharmProjects/deepmd_analysis/deepmd_test/bin/../lib/deepmd/Model.py:888: RuntimeWarning: invalid value encountered in double_scalars
  davgunit = [sumr[type_i]/sumn[type_i], 0, 0, 0]
/home/aurora/PycharmProjects/deepmd_analysis/deepmd_test/bin/../lib/deepmd/Model.py:848: RuntimeWarning: invalid value encountered in double_scalars
  return np.sqrt(sumv2/sumn - np.multiply(sumv/sumn, sumv/sumn))
